{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECE 276C HW3 P2\n",
    "Mingwei Xu A53270271"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gym\n",
    "import pybulletgym.envs\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.distributions import MultivariateNormal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device : cpu\n"
     ]
    }
   ],
   "source": [
    "# setup device\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device('cpu')    # CPU seems faster in this question\n",
    "print('Using device :', device)\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Policy Network using MLP\n",
    "    \"\"\"\n",
    "    def __init__(self, env):\n",
    "        \"\"\"\n",
    "        :param env: object, gym environment\n",
    "        \"\"\"\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        # get state space and action space dimension\n",
    "        self.state_space_n = env.observation_space.shape[0] - 1   # should be 8 (TODO: bug in env showing wrong observation space?)\n",
    "        self.action_space_n = env.action_space.shape[0]   # should be 2\n",
    "\n",
    "        # define layers\n",
    "        self.l1 = nn.Linear(self.state_space_n, 128)\n",
    "        self.l2 = nn.Linear(128, 64)\n",
    "        self.l3 = nn.Linear(64, self.action_space_n)\n",
    "\n",
    "#         self.sigma = nn.Parameter(torch.eye(2))     # initalize cov matrix with grad fn\n",
    "        self.sigma = nn.Parameter(torch.diag([0.2, 0.2]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Feed forward\n",
    "        \n",
    "        :param x: np array, state\n",
    "        :return: tensor, softmax probability of action\n",
    "        \"\"\"\n",
    "        # TODO: take sigma as input\n",
    "        # build neural network\n",
    "        network = nn.Sequential(\n",
    "            self.l1,\n",
    "            nn.Tanh(),\n",
    "            self.l2,\n",
    "            nn.Tanh(),\n",
    "            self.l3,\n",
    "            nn.Tanh())\n",
    "\n",
    "        return network(torch.FloatTensor(x).to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(policy_network, state, eval_policy=False):\n",
    "    \"\"\"\n",
    "    Choose action according to policy on given state\n",
    "\n",
    "    :param policy_network: object, policy network\n",
    "    :param state: np array, state\n",
    "    :param eval_policy: bool, flag to turn on when evaluating policy.\n",
    "                        It will disable sample and return action directly from policy network output.\n",
    "    :returns: list (len=2), action; tensor with grad fn, log probability\n",
    "    \"\"\"\n",
    "    probs = policy_network.forward(state)   # mean from policy network output\n",
    "\n",
    "    cov = torch.abs(policy_network.sigma) + 1e-3    # positive definite\n",
    "\n",
    "    m = MultivariateNormal(probs, cov)\n",
    "    action = m.sample()\n",
    "    log_prob = m.log_prob(action)\n",
    "    \n",
    "    if eval_policy:\n",
    "        print('action: ', probs)\n",
    "        return probs.tolist(), log_prob\n",
    "\n",
    "    return action.tolist(), log_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reinforce(env, policy_network, batch_size=500, num_episodes=200, lr=0.01, gamma=0.99):\n",
    "    \"\"\"\n",
    "    Policy gradient training using reinforce method\n",
    "\n",
    "    :param env: object, gym environment\n",
    "    :param policy_network: object, policy network\n",
    "    :param batch_size: int, batch size\n",
    "    :param num_episodes: int, number of episodes\n",
    "    :param lr: float, learning rate\n",
    "    :param gamma: float (0~1), discount factor\n",
    "    :return: list of average reward on each episode\n",
    "    \"\"\"\n",
    "    # setup place holders\n",
    "    average_reward_list = []  # store step over episode\n",
    "\n",
    "    # define optimizer\n",
    "    optimizer = optim.Adam(policy_network.parameters(), \n",
    "                           lr=lr)\n",
    "\n",
    "    # train\n",
    "    for i in range(num_episodes):\n",
    "        # setup placeholders for each batch\n",
    "        batch_loss_sum = 0\n",
    "        batch_traj_counter = 0\n",
    "        batch_rewards = []\n",
    "\n",
    "        # setup placeholders for each trajectory\n",
    "        traj_rewards = []\n",
    "        traj_log_prob_sum = 0\n",
    "\n",
    "        # reset environment\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "\n",
    "        # batch\n",
    "        for step in range(batch_size):\n",
    "            # exploration\n",
    "            action, log_prob = choose_action(policy_network, state)\n",
    "            state_next, reward, done, _ = env.step(action)\n",
    "            # env.render()\n",
    "\n",
    "            # store data\n",
    "            traj_rewards.append(reward)\n",
    "            traj_log_prob_sum += log_prob\n",
    "\n",
    "            # move on\n",
    "            if done or (step == batch_size - 1):\n",
    "                # trajectory or batch finished, update trajectory\n",
    "                traj_discounted_return = torch.sum(torch.FloatTensor(traj_rewards).to(device) *\n",
    "                                                   torch.FloatTensor([gamma ** t for t in range(1, len(traj_rewards) + 1)]).to(device))  # G(t)\n",
    "                batch_loss_sum += traj_discounted_return * traj_log_prob_sum\n",
    "                \n",
    "                # reset state\n",
    "                batch_rewards.append(np.sum(traj_rewards))\n",
    "                batch_traj_counter += 1\n",
    "\n",
    "                traj_rewards = []\n",
    "                traj_log_prob_sum = 0\n",
    "\n",
    "                state = env.reset()\n",
    "            else:\n",
    "                state = state_next\n",
    "\n",
    "        # finish batch\n",
    "        average_batch_reward = np.mean(batch_rewards)\n",
    "        average_reward_list.append(average_batch_reward)\n",
    "        loss = - batch_loss_sum / batch_traj_counter\n",
    "        \n",
    "        print('Episode [{}/{}] loss: {:.2f}, average reward: {:.2f}, trajectory num: {}'.format(i + 1, num_episodes,\n",
    "                               loss.item(), average_batch_reward, batch_traj_counter))\n",
    "\n",
    "        # update policy\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return average_reward_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reinforce_with_baseline(env, policy_network, batch_size=500, num_episodes=200, lr=0.01, gamma=0.99, enable_baseline=False):\n",
    "    \"\"\"\n",
    "    Policy gradient training using modified reinforce method with baseline\n",
    "\n",
    "    :param env: object, gym environment\n",
    "    :param policy_network: object, policy network\n",
    "    :param batch_size: int, batch size\n",
    "    :param num_episodes: int, number of episodes\n",
    "    :param lr: float, learning rate\n",
    "    :param gamma: float (0~1), discount factor\n",
    "    :param enable_baseline: bool, flag to enable baseline, defaults to False\n",
    "    :return: list of average reward on each episode\n",
    "    \"\"\"\n",
    "    # setup place holders\n",
    "    average_reward_list = []  # store step over episode\n",
    "    average_step_list = []\n",
    "\n",
    "    # define optimizer\n",
    "    optimizer = optim.Adam(policy_network.parameters(), \n",
    "                           lr=lr)\n",
    "\n",
    "    # train\n",
    "    for i in range(num_episodes):\n",
    "        # setup placeholders for each batch\n",
    "        batch_loss_sum = 0\n",
    "        batch_traj_counter = 0\n",
    "        batch_rewards = []\n",
    "        batch_log_prob_list = []\n",
    "        batch_discounted_return_list = []\n",
    "        batch_traj_steps = []\n",
    "\n",
    "        # setup placeholders for each trajectory\n",
    "        traj_rewards = []\n",
    "        traj_log_prob_list = []\n",
    "        traj_step_counter = 0\n",
    "\n",
    "        # reset environment\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "\n",
    "        # batch\n",
    "        for step in range(batch_size):\n",
    "            # exploration\n",
    "            action, log_prob = choose_action(policy_network, state)\n",
    "            state_next, reward, done, _ = env.step(action)\n",
    "            # env.render()\n",
    "\n",
    "            # store data\n",
    "            traj_rewards.append(reward)\n",
    "            traj_log_prob_list.append(log_prob)\n",
    "\n",
    "            # move on\n",
    "            if done or (step == batch_size - 1):\n",
    "                # trajectory or batch finished, update trajectory\n",
    "                discounted_return_list = [sum([gamma ** (t_prime - t) * traj_rewards[t_prime] for t_prime in range(t, len(traj_rewards))]) \\\n",
    "                                          for t in range(1, len(traj_rewards) + 1)]\n",
    "\n",
    "                # collect batch info\n",
    "                batch_log_prob_list.extend(traj_log_prob_list)\n",
    "                batch_discounted_return_list.extend(discounted_return_list)\n",
    "                \n",
    "                # reset state\n",
    "                batch_rewards.append(np.sum(traj_rewards))\n",
    "                batch_traj_counter += 1\n",
    "                batch_traj_steps.append(traj_step_counter)\n",
    "\n",
    "                traj_step_counter = 0\n",
    "                traj_rewards = []\n",
    "                traj_log_prob_list = []\n",
    "\n",
    "                state = env.reset()\n",
    "            else:\n",
    "                state = state_next\n",
    "\n",
    "        # finish batch\n",
    "        # subtract average returns if baseline is enabled\n",
    "        if enable_baseline:\n",
    "            batch_discounted_return_list -= np.mean(batch_discounted_return_list)\n",
    "        \n",
    "        # sum the traj loss by loop so we do not lose tensor gradient\n",
    "        for step in range(len(batch_log_prob_list)):\n",
    "            batch_loss_sum += batch_log_prob_list[step] * batch_discounted_return_list[step]\n",
    "        \n",
    "        average_batch_reward = np.mean(batch_rewards)\n",
    "        average_reward_list.append(average_batch_reward)\n",
    "        average_step_list.append(np.mean(batch_traj_steps))\n",
    "        loss = - batch_loss_sum / batch_traj_counter    # TODO\n",
    "        \n",
    "        print('TODO: sigma: ', policy_network.sigma)    # TODO\n",
    "        print('Episode [{}/{}] loss: {:.2f}, average reward: {:.2f}, trajectory num: {}'.format(i + 1, num_episodes,\n",
    "                               loss.item(), average_batch_reward, batch_traj_counter))\n",
    "\n",
    "        # update policy\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return average_reward_list, average_step_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_dir=/Users/xumw1996/anaconda3/lib/python3.7/site-packages/pybullet_envs/bullet\n",
      "options= \n",
      "[ 0.3928371   0.3928371  -0.68091764  0.26561381  0.5         0.\n",
      "  0.08333333  0.        ]\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1., 0.],\n",
      "        [0., 1.]], requires_grad=True)\n",
      "Episode [1/500] loss: 3.82, average reward: -55.79, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0100, 0.0000],\n",
      "        [0.0000, 1.0100]], requires_grad=True)\n",
      "Episode [2/500] loss: -2.08, average reward: -76.27, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0049, 0.0000],\n",
      "        [0.0000, 1.0191]], requires_grad=True)\n",
      "Episode [3/500] loss: 1.84, average reward: -80.79, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0031, 0.0000],\n",
      "        [0.0000, 1.0273]], requires_grad=True)\n",
      "Episode [4/500] loss: 2.12, average reward: -77.96, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0045, 0.0000],\n",
      "        [0.0000, 1.0346]], requires_grad=True)\n",
      "Episode [5/500] loss: -3.14, average reward: -78.94, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0059, 0.0000],\n",
      "        [0.0000, 1.0349]], requires_grad=True)\n",
      "Episode [6/500] loss: 4.15, average reward: -77.49, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0101, 0.0000],\n",
      "        [0.0000, 1.0363]], requires_grad=True)\n",
      "Episode [7/500] loss: -0.58, average reward: -82.02, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0121, 0.0000],\n",
      "        [0.0000, 1.0386]], requires_grad=True)\n",
      "Episode [8/500] loss: 3.37, average reward: -81.68, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0140, 0.0000],\n",
      "        [0.0000, 1.0430]], requires_grad=True)\n",
      "Episode [9/500] loss: -7.04, average reward: -83.35, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0159, 0.0000],\n",
      "        [0.0000, 1.0416]], requires_grad=True)\n",
      "Episode [10/500] loss: 2.50, average reward: -81.76, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0199, 0.0000],\n",
      "        [0.0000, 1.0400]], requires_grad=True)\n",
      "Episode [11/500] loss: -5.67, average reward: -83.00, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0214, 0.0000],\n",
      "        [0.0000, 1.0371]], requires_grad=True)\n",
      "Episode [12/500] loss: -2.00, average reward: -83.77, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0207, 0.0000],\n",
      "        [0.0000, 1.0351]], requires_grad=True)\n",
      "Episode [13/500] loss: -2.47, average reward: -76.43, trajectory num: 15\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0179, 0.0000],\n",
      "        [0.0000, 1.0338]], requires_grad=True)\n",
      "Episode [14/500] loss: -0.74, average reward: -75.38, trajectory num: 15\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0159, 0.0000],\n",
      "        [0.0000, 1.0319]], requires_grad=True)\n",
      "Episode [15/500] loss: -2.99, average reward: -85.06, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0130, 0.0000],\n",
      "        [0.0000, 1.0297]], requires_grad=True)\n",
      "Episode [16/500] loss: 2.16, average reward: -82.92, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0110, 0.0000],\n",
      "        [0.0000, 1.0282]], requires_grad=True)\n",
      "Episode [17/500] loss: 4.92, average reward: -81.95, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0114, 0.0000],\n",
      "        [0.0000, 1.0275]], requires_grad=True)\n",
      "Episode [18/500] loss: -0.70, average reward: -83.71, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0129, 0.0000],\n",
      "        [0.0000, 1.0255]], requires_grad=True)\n",
      "Episode [19/500] loss: 0.48, average reward: -81.38, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0158, 0.0000],\n",
      "        [0.0000, 1.0225]], requires_grad=True)\n",
      "Episode [20/500] loss: 2.28, average reward: -81.93, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0187, 0.0000],\n",
      "        [0.0000, 1.0205]], requires_grad=True)\n",
      "Episode [21/500] loss: 5.01, average reward: -81.65, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0222, 0.0000],\n",
      "        [0.0000, 1.0203]], requires_grad=True)\n",
      "Episode [22/500] loss: 4.52, average reward: -82.61, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0258, 0.0000],\n",
      "        [0.0000, 1.0218]], requires_grad=True)\n",
      "Episode [23/500] loss: 2.21, average reward: -83.26, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0290, 0.0000],\n",
      "        [0.0000, 1.0240]], requires_grad=True)\n",
      "Episode [24/500] loss: 0.07, average reward: -79.49, trajectory num: 15\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0323, 0.0000],\n",
      "        [0.0000, 1.0258]], requires_grad=True)\n",
      "Episode [25/500] loss: 1.15, average reward: -82.80, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0344, 0.0000],\n",
      "        [0.0000, 1.0285]], requires_grad=True)\n",
      "Episode [26/500] loss: -5.11, average reward: -82.19, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0343, 0.0000],\n",
      "        [0.0000, 1.0306]], requires_grad=True)\n",
      "Episode [27/500] loss: -0.63, average reward: -81.54, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0335, 0.0000],\n",
      "        [0.0000, 1.0327]], requires_grad=True)\n",
      "Episode [28/500] loss: 2.78, average reward: -80.20, trajectory num: 15\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0335, 0.0000],\n",
      "        [0.0000, 1.0353]], requires_grad=True)\n",
      "Episode [29/500] loss: -7.46, average reward: -82.89, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0318, 0.0000],\n",
      "        [0.0000, 1.0359]], requires_grad=True)\n",
      "Episode [30/500] loss: -0.97, average reward: -83.54, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0300, 0.0000],\n",
      "        [0.0000, 1.0362]], requires_grad=True)\n",
      "Episode [31/500] loss: 0.65, average reward: -81.78, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0290, 0.0000],\n",
      "        [0.0000, 1.0362]], requires_grad=True)\n",
      "Episode [32/500] loss: 0.23, average reward: -80.46, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0271, 0.0000],\n",
      "        [0.0000, 1.0373]], requires_grad=True)\n",
      "Episode [33/500] loss: -1.86, average reward: -76.58, trajectory num: 15\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0250, 0.0000],\n",
      "        [0.0000, 1.0377]], requires_grad=True)\n",
      "Episode [34/500] loss: 2.28, average reward: -84.37, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0242, 0.0000],\n",
      "        [0.0000, 1.0381]], requires_grad=True)\n",
      "Episode [35/500] loss: 11.01, average reward: -83.75, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0270, 0.0000],\n",
      "        [0.0000, 1.0396]], requires_grad=True)\n",
      "Episode [36/500] loss: 1.55, average reward: -81.86, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0308, 0.0000],\n",
      "        [0.0000, 1.0402]], requires_grad=True)\n",
      "Episode [37/500] loss: -0.96, average reward: -84.53, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0337, 0.0000],\n",
      "        [0.0000, 1.0410]], requires_grad=True)\n",
      "Episode [38/500] loss: -3.07, average reward: -81.61, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0358, 0.0000],\n",
      "        [0.0000, 1.0409]], requires_grad=True)\n",
      "Episode [39/500] loss: -1.30, average reward: -83.97, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0377, 0.0000],\n",
      "        [0.0000, 1.0402]], requires_grad=True)\n",
      "Episode [40/500] loss: -5.27, average reward: -84.74, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0384, 0.0000],\n",
      "        [0.0000, 1.0385]], requires_grad=True)\n",
      "Episode [41/500] loss: 1.72, average reward: -84.58, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0394, 0.0000],\n",
      "        [0.0000, 1.0372]], requires_grad=True)\n",
      "Episode [42/500] loss: -2.25, average reward: -75.77, trajectory num: 15\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0403, 0.0000],\n",
      "        [0.0000, 1.0350]], requires_grad=True)\n",
      "Episode [43/500] loss: -0.26, average reward: -84.32, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0415, 0.0000],\n",
      "        [0.0000, 1.0326]], requires_grad=True)\n",
      "Episode [44/500] loss: -0.66, average reward: -82.25, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0436, 0.0000],\n",
      "        [0.0000, 1.0290]], requires_grad=True)\n",
      "Episode [45/500] loss: 5.24, average reward: -83.55, trajectory num: 14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0465, 0.0000],\n",
      "        [0.0000, 1.0270]], requires_grad=True)\n",
      "Episode [46/500] loss: -0.85, average reward: -86.45, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0500, 0.0000],\n",
      "        [0.0000, 1.0240]], requires_grad=True)\n",
      "Episode [47/500] loss: -1.81, average reward: -83.43, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0544, 0.0000],\n",
      "        [0.0000, 1.0193]], requires_grad=True)\n",
      "Episode [48/500] loss: -7.79, average reward: -81.17, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0580, 0.0000],\n",
      "        [0.0000, 1.0126]], requires_grad=True)\n",
      "Episode [49/500] loss: -2.15, average reward: -82.63, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0612, 0.0000],\n",
      "        [0.0000, 1.0058]], requires_grad=True)\n",
      "Episode [50/500] loss: -1.59, average reward: -77.58, trajectory num: 15\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0643, 0.0000],\n",
      "        [0.0000, 0.9989]], requires_grad=True)\n",
      "Episode [51/500] loss: -0.56, average reward: -83.19, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0679, 0.0000],\n",
      "        [0.0000, 0.9916]], requires_grad=True)\n",
      "Episode [52/500] loss: -6.12, average reward: -82.68, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0703, 0.0000],\n",
      "        [0.0000, 0.9835]], requires_grad=True)\n",
      "Episode [53/500] loss: -6.95, average reward: -81.16, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0708, 0.0000],\n",
      "        [0.0000, 0.9751]], requires_grad=True)\n",
      "Episode [54/500] loss: -5.41, average reward: -84.09, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0685, 0.0000],\n",
      "        [0.0000, 0.9682]], requires_grad=True)\n",
      "Episode [55/500] loss: -1.82, average reward: -83.06, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0668, 0.0000],\n",
      "        [0.0000, 0.9608]], requires_grad=True)\n",
      "Episode [56/500] loss: 2.80, average reward: -76.57, trajectory num: 15\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0652, 0.0000],\n",
      "        [0.0000, 0.9555]], requires_grad=True)\n",
      "Episode [57/500] loss: -3.07, average reward: -81.61, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0637, 0.0000],\n",
      "        [0.0000, 0.9495]], requires_grad=True)\n",
      "Episode [58/500] loss: 4.87, average reward: -80.85, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0638, 0.0000],\n",
      "        [0.0000, 0.9444]], requires_grad=True)\n",
      "Episode [59/500] loss: 0.67, average reward: -81.92, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0626, 0.0000],\n",
      "        [0.0000, 0.9417]], requires_grad=True)\n",
      "Episode [60/500] loss: 0.28, average reward: -82.79, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0630, 0.0000],\n",
      "        [0.0000, 0.9378]], requires_grad=True)\n",
      "Episode [61/500] loss: 2.82, average reward: -79.00, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0632, 0.0000],\n",
      "        [0.0000, 0.9356]], requires_grad=True)\n",
      "Episode [62/500] loss: 4.07, average reward: -81.79, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0642, 0.0000],\n",
      "        [0.0000, 0.9344]], requires_grad=True)\n",
      "Episode [63/500] loss: 8.07, average reward: -83.51, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0659, 0.0000],\n",
      "        [0.0000, 0.9359]], requires_grad=True)\n",
      "Episode [64/500] loss: -1.24, average reward: -82.88, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0678, 0.0000],\n",
      "        [0.0000, 0.9362]], requires_grad=True)\n",
      "Episode [65/500] loss: 2.78, average reward: -80.35, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0706, 0.0000],\n",
      "        [0.0000, 0.9366]], requires_grad=True)\n",
      "Episode [66/500] loss: -3.88, average reward: -83.53, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0726, 0.0000],\n",
      "        [0.0000, 0.9360]], requires_grad=True)\n",
      "Episode [67/500] loss: 3.73, average reward: -81.40, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0761, 0.0000],\n",
      "        [0.0000, 0.9350]], requires_grad=True)\n",
      "Episode [68/500] loss: -2.10, average reward: -79.20, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0795, 0.0000],\n",
      "        [0.0000, 0.9331]], requires_grad=True)\n",
      "Episode [69/500] loss: -2.47, average reward: -81.37, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0820, 0.0000],\n",
      "        [0.0000, 0.9310]], requires_grad=True)\n",
      "Episode [70/500] loss: -2.71, average reward: -81.80, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0831, 0.0000],\n",
      "        [0.0000, 0.9292]], requires_grad=True)\n",
      "Episode [71/500] loss: 4.64, average reward: -80.63, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0866, 0.0000],\n",
      "        [0.0000, 0.9265]], requires_grad=True)\n",
      "Episode [72/500] loss: 0.51, average reward: -81.37, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0890, 0.0000],\n",
      "        [0.0000, 0.9252]], requires_grad=True)\n",
      "Episode [73/500] loss: -4.45, average reward: -81.86, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0909, 0.0000],\n",
      "        [0.0000, 0.9227]], requires_grad=True)\n",
      "Episode [74/500] loss: 1.17, average reward: -81.89, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0924, 0.0000],\n",
      "        [0.0000, 0.9210]], requires_grad=True)\n",
      "Episode [75/500] loss: -7.40, average reward: -83.74, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0924, 0.0000],\n",
      "        [0.0000, 0.9180]], requires_grad=True)\n",
      "Episode [76/500] loss: 4.76, average reward: -80.20, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0930, 0.0000],\n",
      "        [0.0000, 0.9165]], requires_grad=True)\n",
      "Episode [77/500] loss: 2.84, average reward: -81.84, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0944, 0.0000],\n",
      "        [0.0000, 0.9155]], requires_grad=True)\n",
      "Episode [78/500] loss: -8.26, average reward: -82.76, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0943, 0.0000],\n",
      "        [0.0000, 0.9127]], requires_grad=True)\n",
      "Episode [79/500] loss: -5.45, average reward: -83.05, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0929, 0.0000],\n",
      "        [0.0000, 0.9094]], requires_grad=True)\n",
      "Episode [80/500] loss: 3.61, average reward: -80.75, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0917, 0.0000],\n",
      "        [0.0000, 0.9079]], requires_grad=True)\n",
      "Episode [81/500] loss: 1.71, average reward: -80.48, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0906, 0.0000],\n",
      "        [0.0000, 0.9072]], requires_grad=True)\n",
      "Episode [82/500] loss: -2.53, average reward: -82.91, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0901, 0.0000],\n",
      "        [0.0000, 0.9050]], requires_grad=True)\n",
      "Episode [83/500] loss: 5.00, average reward: -83.58, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0910, 0.0000],\n",
      "        [0.0000, 0.9036]], requires_grad=True)\n",
      "Episode [84/500] loss: 8.56, average reward: -82.56, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0943, 0.0000],\n",
      "        [0.0000, 0.9029]], requires_grad=True)\n",
      "Episode [85/500] loss: -0.19, average reward: -82.75, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0966, 0.0000],\n",
      "        [0.0000, 0.9031]], requires_grad=True)\n",
      "Episode [86/500] loss: 4.89, average reward: -80.67, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0996, 0.0000],\n",
      "        [0.0000, 0.9041]], requires_grad=True)\n",
      "Episode [87/500] loss: -5.01, average reward: -83.05, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1016, 0.0000],\n",
      "        [0.0000, 0.9037]], requires_grad=True)\n",
      "Episode [88/500] loss: -3.87, average reward: -79.95, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1028, 0.0000],\n",
      "        [0.0000, 0.9026]], requires_grad=True)\n",
      "Episode [89/500] loss: -5.54, average reward: -83.32, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1029, 0.0000],\n",
      "        [0.0000, 0.9004]], requires_grad=True)\n",
      "Episode [90/500] loss: -4.09, average reward: -79.13, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1015, 0.0000],\n",
      "        [0.0000, 0.8985]], requires_grad=True)\n",
      "Episode [91/500] loss: 2.27, average reward: -84.59, trajectory num: 14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0998, 0.0000],\n",
      "        [0.0000, 0.8982]], requires_grad=True)\n",
      "Episode [92/500] loss: 11.45, average reward: -81.27, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1021, 0.0000],\n",
      "        [0.0000, 0.8981]], requires_grad=True)\n",
      "Episode [93/500] loss: 0.65, average reward: -80.48, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1042, 0.0000],\n",
      "        [0.0000, 0.8981]], requires_grad=True)\n",
      "Episode [94/500] loss: 2.69, average reward: -79.44, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1073, 0.0000],\n",
      "        [0.0000, 0.8977]], requires_grad=True)\n",
      "Episode [95/500] loss: -3.96, average reward: -80.16, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1097, 0.0000],\n",
      "        [0.0000, 0.8963]], requires_grad=True)\n",
      "Episode [96/500] loss: -2.18, average reward: -75.51, trajectory num: 15\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1119, 0.0000],\n",
      "        [0.0000, 0.8941]], requires_grad=True)\n",
      "Episode [97/500] loss: -5.38, average reward: -83.07, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1125, 0.0000],\n",
      "        [0.0000, 0.8916]], requires_grad=True)\n",
      "Episode [98/500] loss: -12.46, average reward: -83.74, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1100, 0.0000],\n",
      "        [0.0000, 0.8881]], requires_grad=True)\n",
      "Episode [99/500] loss: 2.63, average reward: -81.91, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1076, 0.0000],\n",
      "        [0.0000, 0.8863]], requires_grad=True)\n",
      "Episode [100/500] loss: -1.90, average reward: -82.80, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1049, 0.0000],\n",
      "        [0.0000, 0.8844]], requires_grad=True)\n",
      "Episode [101/500] loss: -2.82, average reward: -82.90, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1023, 0.0000],\n",
      "        [0.0000, 0.8819]], requires_grad=True)\n",
      "Episode [102/500] loss: -2.91, average reward: -81.82, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0988, 0.0000],\n",
      "        [0.0000, 0.8799]], requires_grad=True)\n",
      "Episode [103/500] loss: 0.80, average reward: -83.14, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0960, 0.0000],\n",
      "        [0.0000, 0.8779]], requires_grad=True)\n",
      "Episode [104/500] loss: -7.04, average reward: -80.54, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0929, 0.0000],\n",
      "        [0.0000, 0.8739]], requires_grad=True)\n",
      "Episode [105/500] loss: -0.10, average reward: -81.46, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0901, 0.0000],\n",
      "        [0.0000, 0.8704]], requires_grad=True)\n",
      "Episode [106/500] loss: -8.39, average reward: -85.50, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0854, 0.0000],\n",
      "        [0.0000, 0.8664]], requires_grad=True)\n",
      "Episode [107/500] loss: -6.22, average reward: -81.45, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0808, 0.0000],\n",
      "        [0.0000, 0.8607]], requires_grad=True)\n",
      "Episode [108/500] loss: -4.88, average reward: -83.77, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0769, 0.0000],\n",
      "        [0.0000, 0.8535]], requires_grad=True)\n",
      "Episode [109/500] loss: -2.20, average reward: -83.34, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0735, 0.0000],\n",
      "        [0.0000, 0.8458]], requires_grad=True)\n",
      "Episode [110/500] loss: -1.97, average reward: -81.54, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0708, 0.0000],\n",
      "        [0.0000, 0.8377]], requires_grad=True)\n",
      "Episode [111/500] loss: 10.32, average reward: -79.38, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0688, 0.0000],\n",
      "        [0.0000, 0.8342]], requires_grad=True)\n",
      "Episode [112/500] loss: 0.83, average reward: -79.75, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0674, 0.0000],\n",
      "        [0.0000, 0.8311]], requires_grad=True)\n",
      "Episode [113/500] loss: 3.02, average reward: -82.12, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0669, 0.0000],\n",
      "        [0.0000, 0.8286]], requires_grad=True)\n",
      "Episode [114/500] loss: 0.41, average reward: -77.75, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0661, 0.0000],\n",
      "        [0.0000, 0.8268]], requires_grad=True)\n",
      "Episode [115/500] loss: 4.46, average reward: -80.40, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0660, 0.0000],\n",
      "        [0.0000, 0.8264]], requires_grad=True)\n",
      "Episode [116/500] loss: -6.12, average reward: -79.09, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0648, 0.0000],\n",
      "        [0.0000, 0.8247]], requires_grad=True)\n",
      "Episode [117/500] loss: 1.73, average reward: -82.68, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0645, 0.0000],\n",
      "        [0.0000, 0.8231]], requires_grad=True)\n",
      "Episode [118/500] loss: 2.69, average reward: -81.33, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0644, 0.0000],\n",
      "        [0.0000, 0.8225]], requires_grad=True)\n",
      "Episode [119/500] loss: 1.51, average reward: -79.17, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0641, 0.0000],\n",
      "        [0.0000, 0.8229]], requires_grad=True)\n",
      "Episode [120/500] loss: -0.92, average reward: -80.32, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0629, 0.0000],\n",
      "        [0.0000, 0.8239]], requires_grad=True)\n",
      "Episode [121/500] loss: 0.84, average reward: -82.37, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0614, 0.0000],\n",
      "        [0.0000, 0.8257]], requires_grad=True)\n",
      "Episode [122/500] loss: 7.98, average reward: -80.42, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0596, 0.0000],\n",
      "        [0.0000, 0.8308]], requires_grad=True)\n",
      "Episode [123/500] loss: -2.39, average reward: -79.89, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0568, 0.0000],\n",
      "        [0.0000, 0.8359]], requires_grad=True)\n",
      "Episode [124/500] loss: 2.39, average reward: -81.63, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0551, 0.0000],\n",
      "        [0.0000, 0.8405]], requires_grad=True)\n",
      "Episode [125/500] loss: 4.99, average reward: -80.94, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0551, 0.0000],\n",
      "        [0.0000, 0.8449]], requires_grad=True)\n",
      "Episode [126/500] loss: 3.95, average reward: -79.52, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0566, 0.0000],\n",
      "        [0.0000, 0.8488]], requires_grad=True)\n",
      "Episode [127/500] loss: -3.21, average reward: -82.42, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0572, 0.0000],\n",
      "        [0.0000, 0.8518]], requires_grad=True)\n",
      "Episode [128/500] loss: 0.47, average reward: -79.46, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0580, 0.0000],\n",
      "        [0.0000, 0.8544]], requires_grad=True)\n",
      "Episode [129/500] loss: 1.05, average reward: -82.73, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0595, 0.0000],\n",
      "        [0.0000, 0.8564]], requires_grad=True)\n",
      "Episode [130/500] loss: -1.98, average reward: -78.52, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0595, 0.0000],\n",
      "        [0.0000, 0.8588]], requires_grad=True)\n",
      "Episode [131/500] loss: -0.23, average reward: -79.44, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0595, 0.0000],\n",
      "        [0.0000, 0.8610]], requires_grad=True)\n",
      "Episode [132/500] loss: 0.10, average reward: -82.19, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0580, 0.0000],\n",
      "        [0.0000, 0.8646]], requires_grad=True)\n",
      "Episode [133/500] loss: 10.50, average reward: -78.94, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0593, 0.0000],\n",
      "        [0.0000, 0.8690]], requires_grad=True)\n",
      "Episode [134/500] loss: -2.64, average reward: -80.17, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0593, 0.0000],\n",
      "        [0.0000, 0.8732]], requires_grad=True)\n",
      "Episode [135/500] loss: -1.78, average reward: -77.38, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0593, 0.0000],\n",
      "        [0.0000, 0.8763]], requires_grad=True)\n",
      "Episode [136/500] loss: -5.78, average reward: -79.91, trajectory num: 14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0586, 0.0000],\n",
      "        [0.0000, 0.8778]], requires_grad=True)\n",
      "Episode [137/500] loss: -4.84, average reward: -80.47, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0562, 0.0000],\n",
      "        [0.0000, 0.8792]], requires_grad=True)\n",
      "Episode [138/500] loss: -5.96, average reward: -79.31, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0526, 0.0000],\n",
      "        [0.0000, 0.8798]], requires_grad=True)\n",
      "Episode [139/500] loss: -0.13, average reward: -79.95, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0494, 0.0000],\n",
      "        [0.0000, 0.8802]], requires_grad=True)\n",
      "Episode [140/500] loss: -3.61, average reward: -82.15, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0455, 0.0000],\n",
      "        [0.0000, 0.8804]], requires_grad=True)\n",
      "Episode [141/500] loss: 0.31, average reward: -79.20, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0416, 0.0000],\n",
      "        [0.0000, 0.8811]], requires_grad=True)\n",
      "Episode [142/500] loss: -2.17, average reward: -79.96, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0374, 0.0000],\n",
      "        [0.0000, 0.8816]], requires_grad=True)\n",
      "Episode [143/500] loss: 0.99, average reward: -79.80, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0333, 0.0000],\n",
      "        [0.0000, 0.8829]], requires_grad=True)\n",
      "Episode [144/500] loss: -1.80, average reward: -79.15, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0294, 0.0000],\n",
      "        [0.0000, 0.8835]], requires_grad=True)\n",
      "Episode [145/500] loss: 0.02, average reward: -79.22, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0253, 0.0000],\n",
      "        [0.0000, 0.8847]], requires_grad=True)\n",
      "Episode [146/500] loss: 0.97, average reward: -82.12, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0222, 0.0000],\n",
      "        [0.0000, 0.8855]], requires_grad=True)\n",
      "Episode [147/500] loss: 4.08, average reward: -77.68, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0204, 0.0000],\n",
      "        [0.0000, 0.8868]], requires_grad=True)\n",
      "Episode [148/500] loss: 7.70, average reward: -80.53, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0212, 0.0000],\n",
      "        [0.0000, 0.8882]], requires_grad=True)\n",
      "Episode [149/500] loss: 0.69, average reward: -78.48, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0222, 0.0000],\n",
      "        [0.0000, 0.8895]], requires_grad=True)\n",
      "Episode [150/500] loss: -0.41, average reward: -79.16, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0230, 0.0000],\n",
      "        [0.0000, 0.8906]], requires_grad=True)\n",
      "Episode [151/500] loss: 3.99, average reward: -82.36, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0237, 0.0000],\n",
      "        [0.0000, 0.8932]], requires_grad=True)\n",
      "Episode [152/500] loss: 5.58, average reward: -82.36, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0249, 0.0000],\n",
      "        [0.0000, 0.8970]], requires_grad=True)\n",
      "Episode [153/500] loss: 2.57, average reward: -82.61, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0282, 0.0000],\n",
      "        [0.0000, 0.8990]], requires_grad=True)\n",
      "Episode [154/500] loss: -0.62, average reward: -78.08, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0298, 0.0000],\n",
      "        [0.0000, 0.9021]], requires_grad=True)\n",
      "Episode [155/500] loss: -0.65, average reward: -83.18, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0309, 0.0000],\n",
      "        [0.0000, 0.9050]], requires_grad=True)\n",
      "Episode [156/500] loss: -0.97, average reward: -82.81, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0319, 0.0000],\n",
      "        [0.0000, 0.9072]], requires_grad=True)\n",
      "Episode [157/500] loss: 2.42, average reward: -82.60, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0334, 0.0000],\n",
      "        [0.0000, 0.9094]], requires_grad=True)\n",
      "Episode [158/500] loss: 3.03, average reward: -79.45, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0347, 0.0000],\n",
      "        [0.0000, 0.9128]], requires_grad=True)\n",
      "Episode [159/500] loss: 6.49, average reward: -82.50, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0369, 0.0000],\n",
      "        [0.0000, 0.9169]], requires_grad=True)\n",
      "Episode [160/500] loss: -7.53, average reward: -83.16, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0383, 0.0000],\n",
      "        [0.0000, 0.9186]], requires_grad=True)\n",
      "Episode [161/500] loss: 5.29, average reward: -78.72, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0402, 0.0000],\n",
      "        [0.0000, 0.9214]], requires_grad=True)\n",
      "Episode [162/500] loss: -1.84, average reward: -81.52, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0404, 0.0000],\n",
      "        [0.0000, 0.9247]], requires_grad=True)\n",
      "Episode [163/500] loss: 10.20, average reward: -81.51, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0418, 0.0000],\n",
      "        [0.0000, 0.9302]], requires_grad=True)\n",
      "Episode [164/500] loss: 4.60, average reward: -80.89, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0448, 0.0000],\n",
      "        [0.0000, 0.9348]], requires_grad=True)\n",
      "Episode [165/500] loss: 3.22, average reward: -82.07, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0482, 0.0000],\n",
      "        [0.0000, 0.9395]], requires_grad=True)\n",
      "Episode [166/500] loss: -2.92, average reward: -80.29, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0494, 0.0000],\n",
      "        [0.0000, 0.9447]], requires_grad=True)\n",
      "Episode [167/500] loss: -2.07, average reward: -83.47, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0498, 0.0000],\n",
      "        [0.0000, 0.9493]], requires_grad=True)\n",
      "Episode [168/500] loss: 5.49, average reward: -82.69, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0498, 0.0000],\n",
      "        [0.0000, 0.9556]], requires_grad=True)\n",
      "Episode [169/500] loss: -7.07, average reward: -82.30, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0491, 0.0000],\n",
      "        [0.0000, 0.9595]], requires_grad=True)\n",
      "Episode [170/500] loss: -0.54, average reward: -78.50, trajectory num: 15\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0480, 0.0000],\n",
      "        [0.0000, 0.9634]], requires_grad=True)\n",
      "Episode [171/500] loss: -2.39, average reward: -83.12, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0454, 0.0000],\n",
      "        [0.0000, 0.9677]], requires_grad=True)\n",
      "Episode [172/500] loss: 2.61, average reward: -84.08, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0441, 0.0000],\n",
      "        [0.0000, 0.9714]], requires_grad=True)\n",
      "Episode [173/500] loss: 1.90, average reward: -81.51, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0433, 0.0000],\n",
      "        [0.0000, 0.9750]], requires_grad=True)\n",
      "Episode [174/500] loss: -0.11, average reward: -80.75, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0416, 0.0000],\n",
      "        [0.0000, 0.9792]], requires_grad=True)\n",
      "Episode [175/500] loss: 4.98, average reward: -83.03, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0406, 0.0000],\n",
      "        [0.0000, 0.9842]], requires_grad=True)\n",
      "Episode [176/500] loss: -1.30, average reward: -82.91, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0406, 0.0000],\n",
      "        [0.0000, 0.9872]], requires_grad=True)\n",
      "Episode [177/500] loss: 3.81, average reward: -82.95, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0415, 0.0000],\n",
      "        [0.0000, 0.9905]], requires_grad=True)\n",
      "Episode [178/500] loss: 2.33, average reward: -82.07, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0443, 0.0000],\n",
      "        [0.0000, 0.9922]], requires_grad=True)\n",
      "Episode [179/500] loss: -0.78, average reward: -81.82, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0465, 0.0000],\n",
      "        [0.0000, 0.9939]], requires_grad=True)\n",
      "Episode [180/500] loss: 2.80, average reward: -81.84, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0496, 0.0000],\n",
      "        [0.0000, 0.9951]], requires_grad=True)\n",
      "Episode [181/500] loss: -1.16, average reward: -83.77, trajectory num: 14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0522, 0.0000],\n",
      "        [0.0000, 0.9961]], requires_grad=True)\n",
      "Episode [182/500] loss: -2.97, average reward: -78.91, trajectory num: 15\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0546, 0.0000],\n",
      "        [0.0000, 0.9959]], requires_grad=True)\n",
      "Episode [183/500] loss: -1.04, average reward: -79.83, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0570, 0.0000],\n",
      "        [0.0000, 0.9952]], requires_grad=True)\n",
      "Episode [184/500] loss: -6.97, average reward: -86.08, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0574, 0.0000],\n",
      "        [0.0000, 0.9939]], requires_grad=True)\n",
      "Episode [185/500] loss: -0.67, average reward: -82.63, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0584, 0.0000],\n",
      "        [0.0000, 0.9919]], requires_grad=True)\n",
      "Episode [186/500] loss: -3.91, average reward: -81.15, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0593, 0.0000],\n",
      "        [0.0000, 0.9888]], requires_grad=True)\n",
      "Episode [187/500] loss: 0.04, average reward: -83.66, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0594, 0.0000],\n",
      "        [0.0000, 0.9867]], requires_grad=True)\n",
      "Episode [188/500] loss: 0.32, average reward: -82.66, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0595, 0.0000],\n",
      "        [0.0000, 0.9849]], requires_grad=True)\n",
      "Episode [189/500] loss: 2.69, average reward: -82.53, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0603, 0.0000],\n",
      "        [0.0000, 0.9834]], requires_grad=True)\n",
      "Episode [190/500] loss: 6.29, average reward: -76.96, trajectory num: 15\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0619, 0.0000],\n",
      "        [0.0000, 0.9835]], requires_grad=True)\n",
      "Episode [191/500] loss: -0.17, average reward: -76.20, trajectory num: 15\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0627, 0.0000],\n",
      "        [0.0000, 0.9841]], requires_grad=True)\n",
      "Episode [192/500] loss: 3.21, average reward: -81.77, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0645, 0.0000],\n",
      "        [0.0000, 0.9847]], requires_grad=True)\n",
      "Episode [193/500] loss: -0.60, average reward: -83.41, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0658, 0.0000],\n",
      "        [0.0000, 0.9853]], requires_grad=True)\n",
      "Episode [194/500] loss: -0.24, average reward: -80.06, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0666, 0.0000],\n",
      "        [0.0000, 0.9861]], requires_grad=True)\n",
      "Episode [195/500] loss: 12.56, average reward: -82.64, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0693, 0.0000],\n",
      "        [0.0000, 0.9891]], requires_grad=True)\n",
      "Episode [196/500] loss: -5.22, average reward: -83.76, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0703, 0.0000],\n",
      "        [0.0000, 0.9915]], requires_grad=True)\n",
      "Episode [197/500] loss: -5.29, average reward: -84.02, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0699, 0.0000],\n",
      "        [0.0000, 0.9931]], requires_grad=True)\n",
      "Episode [198/500] loss: 2.00, average reward: -83.26, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0695, 0.0000],\n",
      "        [0.0000, 0.9954]], requires_grad=True)\n",
      "Episode [199/500] loss: 1.03, average reward: -83.51, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0701, 0.0000],\n",
      "        [0.0000, 0.9969]], requires_grad=True)\n",
      "Episode [200/500] loss: 1.82, average reward: -85.19, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0714, 0.0000],\n",
      "        [0.0000, 0.9980]], requires_grad=True)\n",
      "Episode [201/500] loss: -1.06, average reward: -76.24, trajectory num: 15\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0722, 0.0000],\n",
      "        [0.0000, 0.9990]], requires_grad=True)\n",
      "Episode [202/500] loss: 0.21, average reward: -82.87, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0737, 0.0000],\n",
      "        [0.0000, 0.9992]], requires_grad=True)\n",
      "Episode [203/500] loss: -5.46, average reward: -81.94, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0737, 0.0000],\n",
      "        [0.0000, 0.9989]], requires_grad=True)\n",
      "Episode [204/500] loss: 1.45, average reward: -83.25, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0743, 0.0000],\n",
      "        [0.0000, 0.9984]], requires_grad=True)\n",
      "Episode [205/500] loss: 1.62, average reward: -83.07, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0761, 0.0000],\n",
      "        [0.0000, 0.9974]], requires_grad=True)\n",
      "Episode [206/500] loss: 0.37, average reward: -85.32, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0779, 0.0000],\n",
      "        [0.0000, 0.9963]], requires_grad=True)\n",
      "Episode [207/500] loss: 2.45, average reward: -75.41, trajectory num: 15\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0798, 0.0000],\n",
      "        [0.0000, 0.9960]], requires_grad=True)\n",
      "Episode [208/500] loss: -4.45, average reward: -82.99, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0800, 0.0000],\n",
      "        [0.0000, 0.9956]], requires_grad=True)\n",
      "Episode [209/500] loss: 4.78, average reward: -77.13, trajectory num: 15\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0806, 0.0000],\n",
      "        [0.0000, 0.9966]], requires_grad=True)\n",
      "Episode [210/500] loss: 1.93, average reward: -83.39, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0811, 0.0000],\n",
      "        [0.0000, 0.9981]], requires_grad=True)\n",
      "Episode [211/500] loss: -5.69, average reward: -84.64, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0815, 0.0000],\n",
      "        [0.0000, 0.9976]], requires_grad=True)\n",
      "Episode [212/500] loss: -5.23, average reward: -81.99, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0802, 0.0000],\n",
      "        [0.0000, 0.9970]], requires_grad=True)\n",
      "Episode [213/500] loss: 6.96, average reward: -85.22, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0821, 0.0000],\n",
      "        [0.0000, 0.9958]], requires_grad=True)\n",
      "Episode [214/500] loss: 5.50, average reward: -82.72, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0842, 0.0000],\n",
      "        [0.0000, 0.9961]], requires_grad=True)\n",
      "Episode [215/500] loss: 2.46, average reward: -83.89, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0859, 0.0000],\n",
      "        [0.0000, 0.9974]], requires_grad=True)\n",
      "Episode [216/500] loss: 1.62, average reward: -84.24, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0884, 0.0000],\n",
      "        [0.0000, 0.9983]], requires_grad=True)\n",
      "Episode [217/500] loss: -1.91, average reward: -80.99, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0908, 0.0000],\n",
      "        [0.0000, 0.9981]], requires_grad=True)\n",
      "Episode [218/500] loss: -0.87, average reward: -83.52, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0933, 0.0000],\n",
      "        [0.0000, 0.9973]], requires_grad=True)\n",
      "Episode [219/500] loss: 0.12, average reward: -83.09, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0960, 0.0000],\n",
      "        [0.0000, 0.9962]], requires_grad=True)\n",
      "Episode [220/500] loss: -4.64, average reward: -84.22, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.0978, 0.0000],\n",
      "        [0.0000, 0.9942]], requires_grad=True)\n",
      "Episode [221/500] loss: 6.71, average reward: -82.51, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1005, 0.0000],\n",
      "        [0.0000, 0.9937]], requires_grad=True)\n",
      "Episode [222/500] loss: -4.07, average reward: -82.71, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1024, 0.0000],\n",
      "        [0.0000, 0.9923]], requires_grad=True)\n",
      "Episode [223/500] loss: -9.72, average reward: -84.31, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1023, 0.0000],\n",
      "        [0.0000, 0.9897]], requires_grad=True)\n",
      "Episode [224/500] loss: -0.25, average reward: -78.39, trajectory num: 15\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1037, 0.0000],\n",
      "        [0.0000, 0.9856]], requires_grad=True)\n",
      "Episode [225/500] loss: 3.10, average reward: -83.64, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1048, 0.0000],\n",
      "        [0.0000, 0.9832]], requires_grad=True)\n",
      "Episode [226/500] loss: -1.40, average reward: -84.95, trajectory num: 14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1051, 0.0000],\n",
      "        [0.0000, 0.9812]], requires_grad=True)\n",
      "Episode [227/500] loss: 6.94, average reward: -83.15, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1073, 0.0000],\n",
      "        [0.0000, 0.9799]], requires_grad=True)\n",
      "Episode [228/500] loss: -0.69, average reward: -81.33, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1081, 0.0000],\n",
      "        [0.0000, 0.9796]], requires_grad=True)\n",
      "Episode [229/500] loss: -9.11, average reward: -85.47, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1075, 0.0000],\n",
      "        [0.0000, 0.9777]], requires_grad=True)\n",
      "Episode [230/500] loss: -2.30, average reward: -83.13, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1061, 0.0000],\n",
      "        [0.0000, 0.9760]], requires_grad=True)\n",
      "Episode [231/500] loss: 2.25, average reward: -81.52, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1051, 0.0000],\n",
      "        [0.0000, 0.9751]], requires_grad=True)\n",
      "Episode [232/500] loss: -3.43, average reward: -82.28, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1039, 0.0000],\n",
      "        [0.0000, 0.9733]], requires_grad=True)\n",
      "Episode [233/500] loss: 4.55, average reward: -79.30, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1040, 0.0000],\n",
      "        [0.0000, 0.9721]], requires_grad=True)\n",
      "Episode [234/500] loss: -5.64, average reward: -79.88, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1040, 0.0000],\n",
      "        [0.0000, 0.9690]], requires_grad=True)\n",
      "Episode [235/500] loss: 11.16, average reward: -81.27, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1058, 0.0000],\n",
      "        [0.0000, 0.9683]], requires_grad=True)\n",
      "Episode [236/500] loss: -0.66, average reward: -83.18, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1082, 0.0000],\n",
      "        [0.0000, 0.9666]], requires_grad=True)\n",
      "Episode [237/500] loss: -0.26, average reward: -82.34, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1100, 0.0000],\n",
      "        [0.0000, 0.9653]], requires_grad=True)\n",
      "Episode [238/500] loss: 3.06, average reward: -84.30, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1117, 0.0000],\n",
      "        [0.0000, 0.9652]], requires_grad=True)\n",
      "Episode [239/500] loss: 4.87, average reward: -82.98, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1146, 0.0000],\n",
      "        [0.0000, 0.9653]], requires_grad=True)\n",
      "Episode [240/500] loss: 2.95, average reward: -74.91, trajectory num: 15\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1185, 0.0000],\n",
      "        [0.0000, 0.9650]], requires_grad=True)\n",
      "Episode [241/500] loss: 6.00, average reward: -84.29, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1235, 0.0000],\n",
      "        [0.0000, 0.9651]], requires_grad=True)\n",
      "Episode [242/500] loss: 5.99, average reward: -82.51, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1285, 0.0000],\n",
      "        [0.0000, 0.9669]], requires_grad=True)\n",
      "Episode [243/500] loss: -2.91, average reward: -84.58, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1324, 0.0000],\n",
      "        [0.0000, 0.9681]], requires_grad=True)\n",
      "Episode [244/500] loss: 4.43, average reward: -83.40, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1371, 0.0000],\n",
      "        [0.0000, 0.9695]], requires_grad=True)\n",
      "Episode [245/500] loss: 7.25, average reward: -78.12, trajectory num: 15\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1419, 0.0000],\n",
      "        [0.0000, 0.9726]], requires_grad=True)\n",
      "Episode [246/500] loss: -2.81, average reward: -83.17, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1453, 0.0000],\n",
      "        [0.0000, 0.9755]], requires_grad=True)\n",
      "Episode [247/500] loss: -6.07, average reward: -85.15, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1492, 0.0000],\n",
      "        [0.0000, 0.9749]], requires_grad=True)\n",
      "Episode [248/500] loss: 10.29, average reward: -84.29, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1549, 0.0000],\n",
      "        [0.0000, 0.9756]], requires_grad=True)\n",
      "Episode [249/500] loss: -2.64, average reward: -85.65, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1589, 0.0000],\n",
      "        [0.0000, 0.9765]], requires_grad=True)\n",
      "Episode [250/500] loss: -4.52, average reward: -83.33, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1603, 0.0000],\n",
      "        [0.0000, 0.9782]], requires_grad=True)\n",
      "Episode [251/500] loss: -3.82, average reward: -84.10, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1613, 0.0000],\n",
      "        [0.0000, 0.9788]], requires_grad=True)\n",
      "Episode [252/500] loss: -5.93, average reward: -85.59, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1612, 0.0000],\n",
      "        [0.0000, 0.9784]], requires_grad=True)\n",
      "Episode [253/500] loss: 3.40, average reward: -85.61, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1614, 0.0000],\n",
      "        [0.0000, 0.9788]], requires_grad=True)\n",
      "Episode [254/500] loss: -7.75, average reward: -84.25, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1612, 0.0000],\n",
      "        [0.0000, 0.9771]], requires_grad=True)\n",
      "Episode [255/500] loss: -5.97, average reward: -85.02, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1608, 0.0000],\n",
      "        [0.0000, 0.9737]], requires_grad=True)\n",
      "Episode [256/500] loss: -1.12, average reward: -84.89, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1612, 0.0000],\n",
      "        [0.0000, 0.9694]], requires_grad=True)\n",
      "Episode [257/500] loss: 3.80, average reward: -83.02, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1625, 0.0000],\n",
      "        [0.0000, 0.9658]], requires_grad=True)\n",
      "Episode [258/500] loss: -4.06, average reward: -83.46, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1634, 0.0000],\n",
      "        [0.0000, 0.9614]], requires_grad=True)\n",
      "Episode [259/500] loss: -3.03, average reward: -83.02, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1647, 0.0000],\n",
      "        [0.0000, 0.9559]], requires_grad=True)\n",
      "Episode [260/500] loss: 5.71, average reward: -77.75, trajectory num: 15\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1663, 0.0000],\n",
      "        [0.0000, 0.9526]], requires_grad=True)\n",
      "Episode [261/500] loss: -3.71, average reward: -84.65, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1665, 0.0000],\n",
      "        [0.0000, 0.9496]], requires_grad=True)\n",
      "Episode [262/500] loss: 4.00, average reward: -83.53, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1684, 0.0000],\n",
      "        [0.0000, 0.9463]], requires_grad=True)\n",
      "Episode [263/500] loss: -0.44, average reward: -85.88, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1706, 0.0000],\n",
      "        [0.0000, 0.9427]], requires_grad=True)\n",
      "Episode [264/500] loss: -5.44, average reward: -81.14, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1716, 0.0000],\n",
      "        [0.0000, 0.9385]], requires_grad=True)\n",
      "Episode [265/500] loss: 3.63, average reward: -85.28, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1735, 0.0000],\n",
      "        [0.0000, 0.9351]], requires_grad=True)\n",
      "Episode [266/500] loss: -0.42, average reward: -84.55, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1758, 0.0000],\n",
      "        [0.0000, 0.9310]], requires_grad=True)\n",
      "Episode [267/500] loss: -6.90, average reward: -83.19, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1761, 0.0000],\n",
      "        [0.0000, 0.9270]], requires_grad=True)\n",
      "Episode [268/500] loss: -1.62, average reward: -84.65, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1761, 0.0000],\n",
      "        [0.0000, 0.9231]], requires_grad=True)\n",
      "Episode [269/500] loss: -4.02, average reward: -84.27, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1757, 0.0000],\n",
      "        [0.0000, 0.9187]], requires_grad=True)\n",
      "Episode [270/500] loss: 6.44, average reward: -83.83, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1763, 0.0000],\n",
      "        [0.0000, 0.9158]], requires_grad=True)\n",
      "Episode [271/500] loss: 6.08, average reward: -80.10, trajectory num: 14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1776, 0.0000],\n",
      "        [0.0000, 0.9145]], requires_grad=True)\n",
      "Episode [272/500] loss: -4.00, average reward: -84.26, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1787, 0.0000],\n",
      "        [0.0000, 0.9120]], requires_grad=True)\n",
      "Episode [273/500] loss: -3.35, average reward: -84.55, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1787, 0.0000],\n",
      "        [0.0000, 0.9098]], requires_grad=True)\n",
      "Episode [274/500] loss: -7.34, average reward: -85.28, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1777, 0.0000],\n",
      "        [0.0000, 0.9063]], requires_grad=True)\n",
      "Episode [275/500] loss: 0.13, average reward: -82.70, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1757, 0.0000],\n",
      "        [0.0000, 0.9044]], requires_grad=True)\n",
      "Episode [276/500] loss: 2.82, average reward: -83.09, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1740, 0.0000],\n",
      "        [0.0000, 0.9038]], requires_grad=True)\n",
      "Episode [277/500] loss: 2.84, average reward: -83.12, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1727, 0.0000],\n",
      "        [0.0000, 0.9040]], requires_grad=True)\n",
      "Episode [278/500] loss: 6.05, average reward: -84.38, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1728, 0.0000],\n",
      "        [0.0000, 0.9048]], requires_grad=True)\n",
      "Episode [279/500] loss: 5.20, average reward: -81.91, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1742, 0.0000],\n",
      "        [0.0000, 0.9058]], requires_grad=True)\n",
      "Episode [280/500] loss: -2.05, average reward: -82.88, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1750, 0.0000],\n",
      "        [0.0000, 0.9066]], requires_grad=True)\n",
      "Episode [281/500] loss: 5.37, average reward: -79.31, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1760, 0.0000],\n",
      "        [0.0000, 0.9089]], requires_grad=True)\n",
      "Episode [282/500] loss: 2.77, average reward: -82.73, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1776, 0.0000],\n",
      "        [0.0000, 0.9112]], requires_grad=True)\n",
      "Episode [283/500] loss: -0.85, average reward: -84.58, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1787, 0.0000],\n",
      "        [0.0000, 0.9134]], requires_grad=True)\n",
      "Episode [284/500] loss: 1.10, average reward: -83.53, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1800, 0.0000],\n",
      "        [0.0000, 0.9153]], requires_grad=True)\n",
      "Episode [285/500] loss: -1.47, average reward: -82.44, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1821, 0.0000],\n",
      "        [0.0000, 0.9155]], requires_grad=True)\n",
      "Episode [286/500] loss: 0.98, average reward: -82.90, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1850, 0.0000],\n",
      "        [0.0000, 0.9148]], requires_grad=True)\n",
      "Episode [287/500] loss: -8.10, average reward: -83.59, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1867, 0.0000],\n",
      "        [0.0000, 0.9123]], requires_grad=True)\n",
      "Episode [288/500] loss: -3.80, average reward: -79.43, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1875, 0.0000],\n",
      "        [0.0000, 0.9095]], requires_grad=True)\n",
      "Episode [289/500] loss: -0.89, average reward: -84.07, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1884, 0.0000],\n",
      "        [0.0000, 0.9065]], requires_grad=True)\n",
      "Episode [290/500] loss: 2.48, average reward: -83.59, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1895, 0.0000],\n",
      "        [0.0000, 0.9043]], requires_grad=True)\n",
      "Episode [291/500] loss: -1.49, average reward: -82.90, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1914, 0.0000],\n",
      "        [0.0000, 0.9007]], requires_grad=True)\n",
      "Episode [292/500] loss: 5.23, average reward: -76.23, trajectory num: 15\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1933, 0.0000],\n",
      "        [0.0000, 0.8992]], requires_grad=True)\n",
      "Episode [293/500] loss: -4.31, average reward: -83.02, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1940, 0.0000],\n",
      "        [0.0000, 0.8973]], requires_grad=True)\n",
      "Episode [294/500] loss: 3.53, average reward: -84.32, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1961, 0.0000],\n",
      "        [0.0000, 0.8953]], requires_grad=True)\n",
      "Episode [295/500] loss: 2.70, average reward: -83.37, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1982, 0.0000],\n",
      "        [0.0000, 0.8943]], requires_grad=True)\n",
      "Episode [296/500] loss: -5.15, average reward: -80.68, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1983, 0.0000],\n",
      "        [0.0000, 0.8935]], requires_grad=True)\n",
      "Episode [297/500] loss: 3.41, average reward: -82.67, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1986, 0.0000],\n",
      "        [0.0000, 0.8938]], requires_grad=True)\n",
      "Episode [298/500] loss: 4.54, average reward: -81.19, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1993, 0.0000],\n",
      "        [0.0000, 0.8954]], requires_grad=True)\n",
      "Episode [299/500] loss: 7.32, average reward: -77.21, trajectory num: 15\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2013, 0.0000],\n",
      "        [0.0000, 0.8978]], requires_grad=True)\n",
      "Episode [300/500] loss: 6.11, average reward: -78.63, trajectory num: 15\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2036, 0.0000],\n",
      "        [0.0000, 0.9016]], requires_grad=True)\n",
      "Episode [301/500] loss: 3.29, average reward: -85.72, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2067, 0.0000],\n",
      "        [0.0000, 0.9049]], requires_grad=True)\n",
      "Episode [302/500] loss: 6.57, average reward: -82.50, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2105, 0.0000],\n",
      "        [0.0000, 0.9091]], requires_grad=True)\n",
      "Episode [303/500] loss: -1.50, average reward: -87.21, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2132, 0.0000],\n",
      "        [0.0000, 0.9133]], requires_grad=True)\n",
      "Episode [304/500] loss: -2.73, average reward: -83.31, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2159, 0.0000],\n",
      "        [0.0000, 0.9157]], requires_grad=True)\n",
      "Episode [305/500] loss: -6.67, average reward: -82.55, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2166, 0.0000],\n",
      "        [0.0000, 0.9174]], requires_grad=True)\n",
      "Episode [306/500] loss: 3.00, average reward: -81.61, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2177, 0.0000],\n",
      "        [0.0000, 0.9197]], requires_grad=True)\n",
      "Episode [307/500] loss: -8.96, average reward: -78.99, trajectory num: 15\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2172, 0.0000],\n",
      "        [0.0000, 0.9202]], requires_grad=True)\n",
      "Episode [308/500] loss: 0.65, average reward: -82.72, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2164, 0.0000],\n",
      "        [0.0000, 0.9213]], requires_grad=True)\n",
      "Episode [309/500] loss: -0.61, average reward: -83.52, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2162, 0.0000],\n",
      "        [0.0000, 0.9215]], requires_grad=True)\n",
      "Episode [310/500] loss: 0.56, average reward: -83.65, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2165, 0.0000],\n",
      "        [0.0000, 0.9212]], requires_grad=True)\n",
      "Episode [311/500] loss: 3.15, average reward: -81.99, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2175, 0.0000],\n",
      "        [0.0000, 0.9212]], requires_grad=True)\n",
      "Episode [312/500] loss: 1.22, average reward: -82.13, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2175, 0.0000],\n",
      "        [0.0000, 0.9228]], requires_grad=True)\n",
      "Episode [313/500] loss: -2.71, average reward: -84.44, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2165, 0.0000],\n",
      "        [0.0000, 0.9245]], requires_grad=True)\n",
      "Episode [314/500] loss: -5.97, average reward: -83.81, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2144, 0.0000],\n",
      "        [0.0000, 0.9252]], requires_grad=True)\n",
      "Episode [315/500] loss: 1.87, average reward: -83.62, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2116, 0.0000],\n",
      "        [0.0000, 0.9277]], requires_grad=True)\n",
      "Episode [316/500] loss: 1.70, average reward: -85.78, trajectory num: 14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2087, 0.0000],\n",
      "        [0.0000, 0.9310]], requires_grad=True)\n",
      "Episode [317/500] loss: -5.32, average reward: -84.02, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2049, 0.0000],\n",
      "        [0.0000, 0.9333]], requires_grad=True)\n",
      "Episode [318/500] loss: -2.62, average reward: -81.83, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2014, 0.0000],\n",
      "        [0.0000, 0.9347]], requires_grad=True)\n",
      "Episode [319/500] loss: 2.83, average reward: -83.96, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1983, 0.0000],\n",
      "        [0.0000, 0.9368]], requires_grad=True)\n",
      "Episode [320/500] loss: 0.02, average reward: -82.96, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1952, 0.0000],\n",
      "        [0.0000, 0.9392]], requires_grad=True)\n",
      "Episode [321/500] loss: 2.06, average reward: -83.98, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1932, 0.0000],\n",
      "        [0.0000, 0.9410]], requires_grad=True)\n",
      "Episode [322/500] loss: 1.33, average reward: -83.24, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1912, 0.0000],\n",
      "        [0.0000, 0.9434]], requires_grad=True)\n",
      "Episode [323/500] loss: -2.09, average reward: -83.21, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1884, 0.0000],\n",
      "        [0.0000, 0.9460]], requires_grad=True)\n",
      "Episode [324/500] loss: 0.44, average reward: -81.87, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1866, 0.0000],\n",
      "        [0.0000, 0.9477]], requires_grad=True)\n",
      "Episode [325/500] loss: -4.16, average reward: -82.78, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1843, 0.0000],\n",
      "        [0.0000, 0.9484]], requires_grad=True)\n",
      "Episode [326/500] loss: 3.50, average reward: -82.33, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1837, 0.0000],\n",
      "        [0.0000, 0.9487]], requires_grad=True)\n",
      "Episode [327/500] loss: -3.66, average reward: -85.05, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1824, 0.0000],\n",
      "        [0.0000, 0.9485]], requires_grad=True)\n",
      "Episode [328/500] loss: 0.48, average reward: -85.35, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1811, 0.0000],\n",
      "        [0.0000, 0.9488]], requires_grad=True)\n",
      "Episode [329/500] loss: 0.49, average reward: -81.25, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1800, 0.0000],\n",
      "        [0.0000, 0.9489]], requires_grad=True)\n",
      "Episode [330/500] loss: 2.55, average reward: -82.94, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1799, 0.0000],\n",
      "        [0.0000, 0.9491]], requires_grad=True)\n",
      "Episode [331/500] loss: -3.82, average reward: -84.06, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1791, 0.0000],\n",
      "        [0.0000, 0.9485]], requires_grad=True)\n",
      "Episode [332/500] loss: 4.22, average reward: -84.16, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1788, 0.0000],\n",
      "        [0.0000, 0.9492]], requires_grad=True)\n",
      "Episode [333/500] loss: -5.71, average reward: -85.12, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1769, 0.0000],\n",
      "        [0.0000, 0.9496]], requires_grad=True)\n",
      "Episode [334/500] loss: -2.56, average reward: -81.30, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1751, 0.0000],\n",
      "        [0.0000, 0.9491]], requires_grad=True)\n",
      "Episode [335/500] loss: -0.76, average reward: -83.47, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1729, 0.0000],\n",
      "        [0.0000, 0.9490]], requires_grad=True)\n",
      "Episode [336/500] loss: -1.36, average reward: -82.21, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1705, 0.0000],\n",
      "        [0.0000, 0.9490]], requires_grad=True)\n",
      "Episode [337/500] loss: -3.33, average reward: -83.23, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1674, 0.0000],\n",
      "        [0.0000, 0.9487]], requires_grad=True)\n",
      "Episode [338/500] loss: -1.08, average reward: -82.18, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1633, 0.0000],\n",
      "        [0.0000, 0.9497]], requires_grad=True)\n",
      "Episode [339/500] loss: 4.32, average reward: -83.86, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1610, 0.0000],\n",
      "        [0.0000, 0.9505]], requires_grad=True)\n",
      "Episode [340/500] loss: -3.62, average reward: -84.00, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1590, 0.0000],\n",
      "        [0.0000, 0.9499]], requires_grad=True)\n",
      "Episode [341/500] loss: -1.68, average reward: -84.98, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1578, 0.0000],\n",
      "        [0.0000, 0.9480]], requires_grad=True)\n",
      "Episode [342/500] loss: 1.94, average reward: -87.31, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1564, 0.0000],\n",
      "        [0.0000, 0.9474]], requires_grad=True)\n",
      "Episode [343/500] loss: -0.78, average reward: -78.20, trajectory num: 15\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1561, 0.0000],\n",
      "        [0.0000, 0.9454]], requires_grad=True)\n",
      "Episode [344/500] loss: -5.68, average reward: -83.47, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1540, 0.0000],\n",
      "        [0.0000, 0.9438]], requires_grad=True)\n",
      "Episode [345/500] loss: -1.83, average reward: -82.96, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1511, 0.0000],\n",
      "        [0.0000, 0.9427]], requires_grad=True)\n",
      "Episode [346/500] loss: -3.05, average reward: -84.08, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1471, 0.0000],\n",
      "        [0.0000, 0.9422]], requires_grad=True)\n",
      "Episode [347/500] loss: -0.27, average reward: -81.83, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1439, 0.0000],\n",
      "        [0.0000, 0.9412]], requires_grad=True)\n",
      "Episode [348/500] loss: -1.53, average reward: -83.51, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1418, 0.0000],\n",
      "        [0.0000, 0.9389]], requires_grad=True)\n",
      "Episode [349/500] loss: -0.93, average reward: -77.88, trajectory num: 15\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1402, 0.0000],\n",
      "        [0.0000, 0.9361]], requires_grad=True)\n",
      "Episode [350/500] loss: 2.81, average reward: -81.94, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1389, 0.0000],\n",
      "        [0.0000, 0.9345]], requires_grad=True)\n",
      "Episode [351/500] loss: 0.93, average reward: -85.45, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1373, 0.0000],\n",
      "        [0.0000, 0.9339]], requires_grad=True)\n",
      "Episode [352/500] loss: -2.96, average reward: -79.14, trajectory num: 15\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1368, 0.0000],\n",
      "        [0.0000, 0.9312]], requires_grad=True)\n",
      "Episode [353/500] loss: 0.74, average reward: -82.10, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1371, 0.0000],\n",
      "        [0.0000, 0.9282]], requires_grad=True)\n",
      "Episode [354/500] loss: -5.61, average reward: -82.09, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1379, 0.0000],\n",
      "        [0.0000, 0.9228]], requires_grad=True)\n",
      "Episode [355/500] loss: 4.16, average reward: -83.77, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1405, 0.0000],\n",
      "        [0.0000, 0.9176]], requires_grad=True)\n",
      "Episode [356/500] loss: -2.17, average reward: -82.59, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1434, 0.0000],\n",
      "        [0.0000, 0.9113]], requires_grad=True)\n",
      "Episode [357/500] loss: 7.97, average reward: -83.71, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1472, 0.0000],\n",
      "        [0.0000, 0.9073]], requires_grad=True)\n",
      "Episode [358/500] loss: -3.03, average reward: -81.40, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1496, 0.0000],\n",
      "        [0.0000, 0.9038]], requires_grad=True)\n",
      "Episode [359/500] loss: -4.93, average reward: -83.79, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1500, 0.0000],\n",
      "        [0.0000, 0.9007]], requires_grad=True)\n",
      "Episode [360/500] loss: 1.23, average reward: -84.99, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1507, 0.0000],\n",
      "        [0.0000, 0.8981]], requires_grad=True)\n",
      "Episode [361/500] loss: 4.29, average reward: -82.62, trajectory num: 14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1524, 0.0000],\n",
      "        [0.0000, 0.8961]], requires_grad=True)\n",
      "Episode [362/500] loss: -0.60, average reward: -82.86, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1544, 0.0000],\n",
      "        [0.0000, 0.8934]], requires_grad=True)\n",
      "Episode [363/500] loss: 4.67, average reward: -80.88, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1567, 0.0000],\n",
      "        [0.0000, 0.8923]], requires_grad=True)\n",
      "Episode [364/500] loss: 3.84, average reward: -84.40, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1585, 0.0000],\n",
      "        [0.0000, 0.8930]], requires_grad=True)\n",
      "Episode [365/500] loss: -0.99, average reward: -80.69, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1601, 0.0000],\n",
      "        [0.0000, 0.8934]], requires_grad=True)\n",
      "Episode [366/500] loss: 2.53, average reward: -80.64, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1618, 0.0000],\n",
      "        [0.0000, 0.8944]], requires_grad=True)\n",
      "Episode [367/500] loss: -3.06, average reward: -83.88, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1633, 0.0000],\n",
      "        [0.0000, 0.8941]], requires_grad=True)\n",
      "Episode [368/500] loss: -3.24, average reward: -76.88, trajectory num: 15\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1637, 0.0000],\n",
      "        [0.0000, 0.8937]], requires_grad=True)\n",
      "Episode [369/500] loss: -0.63, average reward: -81.66, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1647, 0.0000],\n",
      "        [0.0000, 0.8924]], requires_grad=True)\n",
      "Episode [370/500] loss: 0.17, average reward: -81.88, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1651, 0.0000],\n",
      "        [0.0000, 0.8919]], requires_grad=True)\n",
      "Episode [371/500] loss: -3.88, average reward: -81.29, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1638, 0.0000],\n",
      "        [0.0000, 0.8919]], requires_grad=True)\n",
      "Episode [372/500] loss: 1.42, average reward: -80.99, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1640, 0.0000],\n",
      "        [0.0000, 0.8908]], requires_grad=True)\n",
      "Episode [373/500] loss: 2.70, average reward: -82.64, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1637, 0.0000],\n",
      "        [0.0000, 0.8914]], requires_grad=True)\n",
      "Episode [374/500] loss: 3.10, average reward: -81.34, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1642, 0.0000],\n",
      "        [0.0000, 0.8923]], requires_grad=True)\n",
      "Episode [375/500] loss: -4.77, average reward: -82.97, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1657, 0.0000],\n",
      "        [0.0000, 0.8900]], requires_grad=True)\n",
      "Episode [376/500] loss: 6.30, average reward: -86.17, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1675, 0.0000],\n",
      "        [0.0000, 0.8898]], requires_grad=True)\n",
      "Episode [377/500] loss: 3.47, average reward: -80.93, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1697, 0.0000],\n",
      "        [0.0000, 0.8903]], requires_grad=True)\n",
      "Episode [378/500] loss: 7.84, average reward: -81.51, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1728, 0.0000],\n",
      "        [0.0000, 0.8923]], requires_grad=True)\n",
      "Episode [379/500] loss: 5.53, average reward: -85.04, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1773, 0.0000],\n",
      "        [0.0000, 0.8942]], requires_grad=True)\n",
      "Episode [380/500] loss: 1.54, average reward: -84.32, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1823, 0.0000],\n",
      "        [0.0000, 0.8954]], requires_grad=True)\n",
      "Episode [381/500] loss: 5.76, average reward: -79.70, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1870, 0.0000],\n",
      "        [0.0000, 0.8983]], requires_grad=True)\n",
      "Episode [382/500] loss: 3.22, average reward: -80.31, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1912, 0.0000],\n",
      "        [0.0000, 0.9021]], requires_grad=True)\n",
      "Episode [383/500] loss: 2.88, average reward: -81.00, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.1958, 0.0000],\n",
      "        [0.0000, 0.9057]], requires_grad=True)\n",
      "Episode [384/500] loss: 3.16, average reward: -83.43, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2009, 0.0000],\n",
      "        [0.0000, 0.9091]], requires_grad=True)\n",
      "Episode [385/500] loss: -1.52, average reward: -84.33, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2049, 0.0000],\n",
      "        [0.0000, 0.9122]], requires_grad=True)\n",
      "Episode [386/500] loss: -2.27, average reward: -84.73, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2080, 0.0000],\n",
      "        [0.0000, 0.9147]], requires_grad=True)\n",
      "Episode [387/500] loss: -6.08, average reward: -82.07, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2107, 0.0000],\n",
      "        [0.0000, 0.9149]], requires_grad=True)\n",
      "Episode [388/500] loss: 3.34, average reward: -84.96, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2121, 0.0000],\n",
      "        [0.0000, 0.9174]], requires_grad=True)\n",
      "Episode [389/500] loss: 5.30, average reward: -78.02, trajectory num: 15\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2140, 0.0000],\n",
      "        [0.0000, 0.9209]], requires_grad=True)\n",
      "Episode [390/500] loss: -0.62, average reward: -83.20, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2157, 0.0000],\n",
      "        [0.0000, 0.9239]], requires_grad=True)\n",
      "Episode [391/500] loss: 1.62, average reward: -84.56, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2167, 0.0000],\n",
      "        [0.0000, 0.9276]], requires_grad=True)\n",
      "Episode [392/500] loss: -3.09, average reward: -84.27, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2168, 0.0000],\n",
      "        [0.0000, 0.9310]], requires_grad=True)\n",
      "Episode [393/500] loss: -1.26, average reward: -84.53, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2161, 0.0000],\n",
      "        [0.0000, 0.9343]], requires_grad=True)\n",
      "Episode [394/500] loss: 0.46, average reward: -84.37, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2151, 0.0000],\n",
      "        [0.0000, 0.9380]], requires_grad=True)\n",
      "Episode [395/500] loss: 0.71, average reward: -83.35, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2142, 0.0000],\n",
      "        [0.0000, 0.9415]], requires_grad=True)\n",
      "Episode [396/500] loss: 7.47, average reward: -85.82, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2142, 0.0000],\n",
      "        [0.0000, 0.9463]], requires_grad=True)\n",
      "Episode [397/500] loss: -1.63, average reward: -85.62, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2143, 0.0000],\n",
      "        [0.0000, 0.9500]], requires_grad=True)\n",
      "Episode [398/500] loss: 1.48, average reward: -86.23, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2143, 0.0000],\n",
      "        [0.0000, 0.9540]], requires_grad=True)\n",
      "Episode [399/500] loss: -5.82, average reward: -84.53, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2131, 0.0000],\n",
      "        [0.0000, 0.9568]], requires_grad=True)\n",
      "Episode [400/500] loss: 7.52, average reward: -85.30, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2120, 0.0000],\n",
      "        [0.0000, 0.9620]], requires_grad=True)\n",
      "Episode [401/500] loss: 8.00, average reward: -82.50, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2127, 0.0000],\n",
      "        [0.0000, 0.9675]], requires_grad=True)\n",
      "Episode [402/500] loss: -0.46, average reward: -85.00, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2137, 0.0000],\n",
      "        [0.0000, 0.9720]], requires_grad=True)\n",
      "Episode [403/500] loss: 3.51, average reward: -78.28, trajectory num: 15\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2142, 0.0000],\n",
      "        [0.0000, 0.9776]], requires_grad=True)\n",
      "Episode [404/500] loss: 1.84, average reward: -84.57, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2159, 0.0000],\n",
      "        [0.0000, 0.9819]], requires_grad=True)\n",
      "Episode [405/500] loss: 7.15, average reward: -84.37, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2178, 0.0000],\n",
      "        [0.0000, 0.9878]], requires_grad=True)\n",
      "Episode [406/500] loss: 4.53, average reward: -85.42, trajectory num: 14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2211, 0.0000],\n",
      "        [0.0000, 0.9929]], requires_grad=True)\n",
      "Episode [407/500] loss: 1.02, average reward: -83.62, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2249, 0.0000],\n",
      "        [0.0000, 0.9969]], requires_grad=True)\n",
      "Episode [408/500] loss: -5.40, average reward: -84.22, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2275, 0.0000],\n",
      "        [0.0000, 0.9997]], requires_grad=True)\n",
      "Episode [409/500] loss: 2.24, average reward: -85.00, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2288, 0.0000],\n",
      "        [0.0000, 1.0039]], requires_grad=True)\n",
      "Episode [410/500] loss: 11.07, average reward: -86.69, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2308, 0.0000],\n",
      "        [0.0000, 1.0105]], requires_grad=True)\n",
      "Episode [411/500] loss: -3.46, average reward: -83.45, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2321, 0.0000],\n",
      "        [0.0000, 1.0158]], requires_grad=True)\n",
      "Episode [412/500] loss: 1.91, average reward: -87.06, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2328, 0.0000],\n",
      "        [0.0000, 1.0217]], requires_grad=True)\n",
      "Episode [413/500] loss: 4.47, average reward: -85.06, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2342, 0.0000],\n",
      "        [0.0000, 1.0276]], requires_grad=True)\n",
      "Episode [414/500] loss: 4.91, average reward: -88.77, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2363, 0.0000],\n",
      "        [0.0000, 1.0336]], requires_grad=True)\n",
      "Episode [415/500] loss: 3.54, average reward: -88.42, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2387, 0.0000],\n",
      "        [0.0000, 1.0396]], requires_grad=True)\n",
      "Episode [416/500] loss: -3.11, average reward: -85.22, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2393, 0.0000],\n",
      "        [0.0000, 1.0456]], requires_grad=True)\n",
      "Episode [417/500] loss: -7.63, average reward: -83.80, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2385, 0.0000],\n",
      "        [0.0000, 1.0500]], requires_grad=True)\n",
      "Episode [418/500] loss: 1.73, average reward: -86.75, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2381, 0.0000],\n",
      "        [0.0000, 1.0541]], requires_grad=True)\n",
      "Episode [419/500] loss: -2.88, average reward: -83.31, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2371, 0.0000],\n",
      "        [0.0000, 1.0576]], requires_grad=True)\n",
      "Episode [420/500] loss: -0.52, average reward: -79.34, trajectory num: 15\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2352, 0.0000],\n",
      "        [0.0000, 1.0616]], requires_grad=True)\n",
      "Episode [421/500] loss: 1.29, average reward: -86.74, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2338, 0.0000],\n",
      "        [0.0000, 1.0654]], requires_grad=True)\n",
      "Episode [422/500] loss: 7.14, average reward: -88.18, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2329, 0.0000],\n",
      "        [0.0000, 1.0705]], requires_grad=True)\n",
      "Episode [423/500] loss: -0.90, average reward: -86.94, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2326, 0.0000],\n",
      "        [0.0000, 1.0743]], requires_grad=True)\n",
      "Episode [424/500] loss: 7.84, average reward: -85.53, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2337, 0.0000],\n",
      "        [0.0000, 1.0788]], requires_grad=True)\n",
      "Episode [425/500] loss: -5.93, average reward: -80.70, trajectory num: 15\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2349, 0.0000],\n",
      "        [0.0000, 1.0809]], requires_grad=True)\n",
      "Episode [426/500] loss: 5.45, average reward: -86.36, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2381, 0.0000],\n",
      "        [0.0000, 1.0823]], requires_grad=True)\n",
      "Episode [427/500] loss: 5.51, average reward: -87.89, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2438, 0.0000],\n",
      "        [0.0000, 1.0824]], requires_grad=True)\n",
      "Episode [428/500] loss: -3.41, average reward: -88.27, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2484, 0.0000],\n",
      "        [0.0000, 1.0821]], requires_grad=True)\n",
      "Episode [429/500] loss: -4.33, average reward: -83.85, trajectory num: 15\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2523, 0.0000],\n",
      "        [0.0000, 1.0807]], requires_grad=True)\n",
      "Episode [430/500] loss: -4.98, average reward: -87.94, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2553, 0.0000],\n",
      "        [0.0000, 1.0785]], requires_grad=True)\n",
      "Episode [431/500] loss: -7.97, average reward: -86.90, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2565, 0.0000],\n",
      "        [0.0000, 1.0756]], requires_grad=True)\n",
      "Episode [432/500] loss: 1.34, average reward: -87.74, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2577, 0.0000],\n",
      "        [0.0000, 1.0733]], requires_grad=True)\n",
      "Episode [433/500] loss: -1.22, average reward: -81.41, trajectory num: 15\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2597, 0.0000],\n",
      "        [0.0000, 1.0699]], requires_grad=True)\n",
      "Episode [434/500] loss: -3.93, average reward: -85.60, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2629, 0.0000],\n",
      "        [0.0000, 1.0642]], requires_grad=True)\n",
      "Episode [435/500] loss: 7.16, average reward: -81.41, trajectory num: 15\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2664, 0.0000],\n",
      "        [0.0000, 1.0608]], requires_grad=True)\n",
      "Episode [436/500] loss: 3.07, average reward: -85.16, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2700, 0.0000],\n",
      "        [0.0000, 1.0581]], requires_grad=True)\n",
      "Episode [437/500] loss: -2.49, average reward: -79.27, trajectory num: 15\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2728, 0.0000],\n",
      "        [0.0000, 1.0554]], requires_grad=True)\n",
      "Episode [438/500] loss: 5.02, average reward: -84.69, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2770, 0.0000],\n",
      "        [0.0000, 1.0527]], requires_grad=True)\n",
      "Episode [439/500] loss: -6.69, average reward: -87.20, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2795, 0.0000],\n",
      "        [0.0000, 1.0497]], requires_grad=True)\n",
      "Episode [440/500] loss: 1.15, average reward: -86.82, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2826, 0.0000],\n",
      "        [0.0000, 1.0463]], requires_grad=True)\n",
      "Episode [441/500] loss: 2.62, average reward: -85.59, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2854, 0.0000],\n",
      "        [0.0000, 1.0441]], requires_grad=True)\n",
      "Episode [442/500] loss: 3.11, average reward: -87.43, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2877, 0.0000],\n",
      "        [0.0000, 1.0434]], requires_grad=True)\n",
      "Episode [443/500] loss: 5.21, average reward: -86.92, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2913, 0.0000],\n",
      "        [0.0000, 1.0427]], requires_grad=True)\n",
      "Episode [444/500] loss: 4.11, average reward: -89.46, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2952, 0.0000],\n",
      "        [0.0000, 1.0426]], requires_grad=True)\n",
      "Episode [445/500] loss: -2.18, average reward: -81.93, trajectory num: 15\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2984, 0.0000],\n",
      "        [0.0000, 1.0422]], requires_grad=True)\n",
      "Episode [446/500] loss: -5.66, average reward: -88.74, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.3012, 0.0000],\n",
      "        [0.0000, 1.0403]], requires_grad=True)\n",
      "Episode [447/500] loss: -12.48, average reward: -88.05, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.3022, 0.0000],\n",
      "        [0.0000, 1.0362]], requires_grad=True)\n",
      "Episode [448/500] loss: 3.10, average reward: -85.68, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.3027, 0.0000],\n",
      "        [0.0000, 1.0339]], requires_grad=True)\n",
      "Episode [449/500] loss: -5.81, average reward: -87.14, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.3017, 0.0000],\n",
      "        [0.0000, 1.0316]], requires_grad=True)\n",
      "Episode [450/500] loss: 2.89, average reward: -88.42, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.3019, 0.0000],\n",
      "        [0.0000, 1.0293]], requires_grad=True)\n",
      "Episode [451/500] loss: -3.21, average reward: -89.69, trajectory num: 14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.3011, 0.0000],\n",
      "        [0.0000, 1.0273]], requires_grad=True)\n",
      "Episode [452/500] loss: 2.23, average reward: -88.34, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.3012, 0.0000],\n",
      "        [0.0000, 1.0252]], requires_grad=True)\n",
      "Episode [453/500] loss: 0.90, average reward: -86.84, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.3011, 0.0000],\n",
      "        [0.0000, 1.0238]], requires_grad=True)\n",
      "Episode [454/500] loss: 6.85, average reward: -90.96, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.3007, 0.0000],\n",
      "        [0.0000, 1.0251]], requires_grad=True)\n",
      "Episode [455/500] loss: -0.78, average reward: -85.96, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.3002, 0.0000],\n",
      "        [0.0000, 1.0262]], requires_grad=True)\n",
      "Episode [456/500] loss: 3.42, average reward: -85.39, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2992, 0.0000],\n",
      "        [0.0000, 1.0288]], requires_grad=True)\n",
      "Episode [457/500] loss: -0.74, average reward: -88.50, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2978, 0.0000],\n",
      "        [0.0000, 1.0314]], requires_grad=True)\n",
      "Episode [458/500] loss: 5.36, average reward: -84.28, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2977, 0.0000],\n",
      "        [0.0000, 1.0342]], requires_grad=True)\n",
      "Episode [459/500] loss: 0.63, average reward: -83.98, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2977, 0.0000],\n",
      "        [0.0000, 1.0368]], requires_grad=True)\n",
      "Episode [460/500] loss: 3.81, average reward: -84.31, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2994, 0.0000],\n",
      "        [0.0000, 1.0385]], requires_grad=True)\n",
      "Episode [461/500] loss: -0.99, average reward: -87.13, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.3003, 0.0000],\n",
      "        [0.0000, 1.0405]], requires_grad=True)\n",
      "Episode [462/500] loss: -6.43, average reward: -87.23, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2996, 0.0000],\n",
      "        [0.0000, 1.0419]], requires_grad=True)\n",
      "Episode [463/500] loss: -6.76, average reward: -88.43, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2975, 0.0000],\n",
      "        [0.0000, 1.0425]], requires_grad=True)\n",
      "Episode [464/500] loss: 4.36, average reward: -86.44, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2960, 0.0000],\n",
      "        [0.0000, 1.0441]], requires_grad=True)\n",
      "Episode [465/500] loss: -8.04, average reward: -82.85, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2937, 0.0000],\n",
      "        [0.0000, 1.0440]], requires_grad=True)\n",
      "Episode [466/500] loss: 12.57, average reward: -89.40, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2953, 0.0000],\n",
      "        [0.0000, 1.0439]], requires_grad=True)\n",
      "Episode [467/500] loss: -7.51, average reward: -88.39, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2958, 0.0000],\n",
      "        [0.0000, 1.0426]], requires_grad=True)\n",
      "Episode [468/500] loss: 3.09, average reward: -83.94, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2965, 0.0000],\n",
      "        [0.0000, 1.0419]], requires_grad=True)\n",
      "Episode [469/500] loss: -1.63, average reward: -87.80, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2964, 0.0000],\n",
      "        [0.0000, 1.0418]], requires_grad=True)\n",
      "Episode [470/500] loss: -5.30, average reward: -83.14, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2950, 0.0000],\n",
      "        [0.0000, 1.0412]], requires_grad=True)\n",
      "Episode [471/500] loss: 6.72, average reward: -87.46, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2956, 0.0000],\n",
      "        [0.0000, 1.0409]], requires_grad=True)\n",
      "Episode [472/500] loss: -7.62, average reward: -88.90, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2954, 0.0000],\n",
      "        [0.0000, 1.0391]], requires_grad=True)\n",
      "Episode [473/500] loss: 2.23, average reward: -86.90, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2949, 0.0000],\n",
      "        [0.0000, 1.0385]], requires_grad=True)\n",
      "Episode [474/500] loss: -0.89, average reward: -86.98, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2942, 0.0000],\n",
      "        [0.0000, 1.0380]], requires_grad=True)\n",
      "Episode [475/500] loss: -4.57, average reward: -88.29, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2929, 0.0000],\n",
      "        [0.0000, 1.0367]], requires_grad=True)\n",
      "Episode [476/500] loss: 2.44, average reward: -87.31, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2917, 0.0000],\n",
      "        [0.0000, 1.0364]], requires_grad=True)\n",
      "Episode [477/500] loss: -2.70, average reward: -87.19, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2901, 0.0000],\n",
      "        [0.0000, 1.0358]], requires_grad=True)\n",
      "Episode [478/500] loss: -6.66, average reward: -79.81, trajectory num: 15\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2872, 0.0000],\n",
      "        [0.0000, 1.0348]], requires_grad=True)\n",
      "Episode [479/500] loss: 4.27, average reward: -88.65, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2862, 0.0000],\n",
      "        [0.0000, 1.0335]], requires_grad=True)\n",
      "Episode [480/500] loss: -9.20, average reward: -88.19, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2836, 0.0000],\n",
      "        [0.0000, 1.0313]], requires_grad=True)\n",
      "Episode [481/500] loss: -1.06, average reward: -82.55, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2808, 0.0000],\n",
      "        [0.0000, 1.0294]], requires_grad=True)\n",
      "Episode [482/500] loss: -0.07, average reward: -83.79, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2792, 0.0000],\n",
      "        [0.0000, 1.0268]], requires_grad=True)\n",
      "Episode [483/500] loss: -2.33, average reward: -86.93, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2775, 0.0000],\n",
      "        [0.0000, 1.0238]], requires_grad=True)\n",
      "Episode [484/500] loss: -4.88, average reward: -83.05, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2760, 0.0000],\n",
      "        [0.0000, 1.0197]], requires_grad=True)\n",
      "Episode [485/500] loss: -2.18, average reward: -86.43, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2740, 0.0000],\n",
      "        [0.0000, 1.0159]], requires_grad=True)\n",
      "Episode [486/500] loss: -7.71, average reward: -86.70, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2710, 0.0000],\n",
      "        [0.0000, 1.0114]], requires_grad=True)\n",
      "Episode [487/500] loss: -6.41, average reward: -84.72, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2682, 0.0000],\n",
      "        [0.0000, 1.0054]], requires_grad=True)\n",
      "Episode [488/500] loss: -0.86, average reward: -83.86, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2672, 0.0000],\n",
      "        [0.0000, 0.9980]], requires_grad=True)\n",
      "Episode [489/500] loss: 10.62, average reward: -84.60, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2681, 0.0000],\n",
      "        [0.0000, 0.9929]], requires_grad=True)\n",
      "Episode [490/500] loss: 0.22, average reward: -85.97, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2683, 0.0000],\n",
      "        [0.0000, 0.9891]], requires_grad=True)\n",
      "Episode [491/500] loss: -2.41, average reward: -86.32, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2672, 0.0000],\n",
      "        [0.0000, 0.9862]], requires_grad=True)\n",
      "Episode [492/500] loss: -0.33, average reward: -85.83, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2656, 0.0000],\n",
      "        [0.0000, 0.9842]], requires_grad=True)\n",
      "Episode [493/500] loss: -1.34, average reward: -85.73, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2649, 0.0000],\n",
      "        [0.0000, 0.9811]], requires_grad=True)\n",
      "Episode [494/500] loss: 1.50, average reward: -82.38, trajectory num: 15\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2641, 0.0000],\n",
      "        [0.0000, 0.9791]], requires_grad=True)\n",
      "Episode [495/500] loss: 2.57, average reward: -87.33, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2639, 0.0000],\n",
      "        [0.0000, 0.9775]], requires_grad=True)\n",
      "Episode [496/500] loss: 6.03, average reward: -86.74, trajectory num: 14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2641, 0.0000],\n",
      "        [0.0000, 0.9777]], requires_grad=True)\n",
      "Episode [497/500] loss: -2.50, average reward: -80.72, trajectory num: 15\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2647, 0.0000],\n",
      "        [0.0000, 0.9765]], requires_grad=True)\n",
      "Episode [498/500] loss: -0.76, average reward: -80.21, trajectory num: 15\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2645, 0.0000],\n",
      "        [0.0000, 0.9760]], requires_grad=True)\n",
      "Episode [499/500] loss: -1.36, average reward: -86.66, trajectory num: 14\n",
      "TODO: sigma:  Parameter containing:\n",
      "tensor([[1.2642, 0.0000],\n",
      "        [0.0000, 0.9753]], requires_grad=True)\n",
      "Episode [500/500] loss: -4.64, average reward: -85.34, trajectory num: 14\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('modified_gym_env:ReacherPyBulletEnv-v1', rand_init=False)\n",
    "# env.render()\n",
    "# TODO: test\n",
    "state = env.reset()\n",
    "print(state)\n",
    "\n",
    "# setup network\n",
    "policy_network = PolicyNetwork(env).to(device)\n",
    "average_reward_list, average_step_list = reinforce_with_baseline(env, policy_network,batch_size=2000, num_episodes=500,\n",
    "                                              lr=0.01, gamma=0.9, enable_baseline=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAEWCAYAAABIVsEJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAesUlEQVR4nO3de7xUdb3/8ddbSMBEBEEDAbcX+nmwi3m2qGX9PIaKVkd/ZaUeDU8Wnn5Z2ulysDp5yV+pXdRSU35pesy8lFlkFyTUrpZslFJCA/MCBApyUfKkop/zx/c7uhhn7z0s9uyZzX4/H495zFrf73dmPt+ZNesza33XrKWIwMzMbFNt1ewAzMysb3ICMTOzUpxAzMysFCcQMzMrxQnEzMxKcQIxM7NSnEBsI5L+RdKtzY7DTNKZkr7d7DjqJWm8pPWSBjQ7lt7iBNIiJJ0o6V5JT0taIelSScMa/JptkkLSwEpZRFwbEYc24LX2lzRb0mpJKyV9V9LoOh53laQN9bTtCySNlXSTpFWS1km6T9KJue5ln0cryJ/BOd20CUl/yyvQyu1TvRVjIYY98nTDk4+khyVNrsxHxKMRsW1EPN/I120lTiAtQNLHgfOATwLDgP2BNuBWSa9oYmg9aTgwg9SvXYCngG919QBJrwTeBawDjm9EUE1YWV8DLCG9BzsAJwCP9XIMjfL6vAKt3M5vdkBltVoSb1kR4VsTb8B2wHrgPVXl2wIrgal5/irgnEL9QcDSwvwY4Kb8mIeAjxbqJgEdwJOkldVXc/mjQOTXXw8cAJwI/Lrw2DcCc0kr8bnAGwt1dwCfB35DSgi3AiPr7Pc+wFPdtHkfaWV7KnBfVV//GxhRKHsDsAp4RZ5/P7AQWAPMAnYptA3gw8Ai4KFcdlF+rSeBecCbC+2HAFfn51oIfKre975Gn9YDe3dS97LPo86+fBT4S+7/l4Ctct0ewC/yZ7cKuKGLuL4LrMhtfwnslcunAc8Bz+aYftTJ4wPYo5O6ScCdwFpgOXAxsHWhfi9gNrA6L5+fzuVnAjcC/5WXrwVAexd9iNznKTne53LMf8j1w4ArcgzLgHOAAbnuRNJyfAHwRK7bHbgtz68CrgW2z+2vAV4gLYfr8zLRlmMYWFguZuZ+LQY+WIh1k/rWqremB9Dfb3lh31BZ6KrqrgauzdNX0UkCIW1JzgM+B2wN7JZXKIfl+juBE/L0tsD+eXqjBT6XnUhOIMAI0krrBGAgcGye3yHX3wE8CLyatJK9Azi3zn6fBvyumzZzgPOBnfJ79I+FutuqvpBfAi7L00fmL+w/5Lg/C/y20DZIK6wRwJBcdjxpi2Ag8HHSynRwrjuXtCIeDowF/ljve1+jTz8nraiOAcZX1dX6POrpy+25L+OBPwMfyHXXAZ/JMQ4GDuzivX4/MBQYBFwIzC/UXUVh2evk8V0lkH8kbVUPzH1cCJyW64aSVugfzzEOBfbLdWcCfweOAAYAX+xqmSnGkB/77ar6m4HLgVcCOwJ3AScXlvsNwEdynENIyeiQ/J6MIiXWCwvP9zAwubPPL7e/NPdrb9IPjIPL9K1Vb00PoL/fSCuuFZ3UnQvcmqc3+hKzcQLZD3i06rGnA9/K078EzqJq66B6gc9lJ/JSAjkBuKvqMXcCJ+bpO4DPFur+L/CzOvr8OtKvsjd30WY86Rfe3nl+FnBRof4DwG15WqSth7fk+Z8CJxXabgU8Tf7lnvt8cDcxriHtkoGqhJBfu673vsbzDs+f6wLgeWA+sG8Xn0c9fZlS9RnMydP/RdptOHYTl8nt8/MOq7XsdfKYIG29rS3cOkuipwE35+ljgXs6aXcm8PPC/ETgv7uJoWYCIf0IeYb8g6Hw2rcXlvtHO3vu3OaoYqx0kUCAcfnzHVqo/yJwVZm+terNYyDNtwoY2ck+19G5vju7AGMkra3cgE+TvjQAJ5G2Eu6XNFfS2+uMbQzwSFXZI8DOhfkVhemnSVs4ncqDnD8FTo2IX3XR9ARgYUTMz/PXAscVxoRuAg7Ig+tvISWbyvPtAlxUeC9Wk5JMMe4lVXF9QtLCPLC9lrS7Y2SuHlPVvjjd3Xu/kYhYExHTI2Kv3GY+8ANJ6uR92NS+PJLjhbRbRcBdkhZIen+tF5A0QNK5kh6U9CRpxUih//XaJyK2L9xm5ed/taRb8sEhTwJfKDz3ONJWbGeql6/BJccndgFeASwvvJeXk7ZEKqqXiZ0kXS9pWY7729T/nowBVkfEU4Wy7r47ZfvWNE4gzXcn6ZfRO4uFkrYFDif9ygf4G7BNocmrCtNLSPvyi1/eoRFxBEBELIqIY0lflvOA7+UB6ugmtr+SvnhF40n7jzeZpF1Iu3A+HxHXdNP8fcBueaWzAvgq6ctb6dMa0pjLe4HjgOsj/5QjvR8nV70fQyLit4Xnf7Hvkt5MWtm+BxgeEduTxgIqK/XlpF1XFeMK012+912JiFXAl0krmxHU/jzq6UsxnvGkz42IWBERH4yIMcDJwKWVo5SqHEfaVTaZlDjbcnml/90tJ935BnA/MCEitiMl2MpzLyHt9utp1TEvIX3PRhbex+1yIu/sMV/IZa/NcR9fiLtW+6K/AiMkDS2Ulf7utConkCaLiHWk3UtflzRF0isktZEG2CoDd5B+qR4haYSkV5F2A1TcBTwl6T8kDcm/KF8jaV8AScdLGhURL5B2LUD6xb4y33f2Bf4J8GpJx0kaKOm9pE3tWza1n5J2Jo1bXBwRl3XT9gDSAOYk0r7jvYHXAN8hJZaKyvzRebriMuB0SXvl5xsm6d1dvORQ0v7vlcBASZ8jHdxQcWN+vuG5H6cU6rp872v07bxcPzCvXD4ELI6IJ6j9edTTl0/m2MaRDji4Ibd9t6RK4ltDWuG90En/nyENFm9DWnEWPcbmreSHknZvrZe0J6nPFbcAoyWdJmmQpKGS9tuM16p4DGiTtBVARCwn/eD4iqTtJG0laXdJ/7ubuNcD6/Ln/skar1HzfYmIJcBvgS9KGizpdaQ9AX3mfy11afY+NN/SjbRw3UcaWAvSlseYQv1g0orhSdIg7sd4+ZFA15E2i9cAvyPvnyUttI+TvgwLgKMKjzubtOJaSxroPJGNj8I6kDRIvC7fH1iou4M8YJvnN3psVf/OYOMjjNYD6ztpexlwU43ySaQV3Yg8P4R8BEuNticA9+b3awlwZaFuowFf0iDmlbntctLWyMOF9++VpKNu1pIGgD8LPFjPe18jrq+Tjv5an9/3W4B/6OzzqLMvlaOwngC+wktHFp1P+sW7nrSbaFonMW0L/DC/l4+QknJxPGEC6QfMWuAHnTxHkLaSi5/vhbnuLaQtkPWk3YxnVy1jryEdMLEmv4fTc/mZbDyO0UbVGFGNGCox7wD8Oj/n3blsGGlraClpeb4HOKazZZd0dNi8HPd80kB/8Tt3JOnIubXAJ6rjI2213kLa7fgg8G+Fx25S31r1phy8tRBJ/0r6kr0pIh5tdjy2MUkfIq14uvr12luxBGnX0OJmx2L9T58asOkvIuJbkjaQ/oPhBNJkeaB+N9J41QTSL9GLmxqUWQtwAmlR0f0gs/WerUlH7OxK2l1xPen4frN+zbuwzMysFB+FZWZmpfSrXVgjR46Mtra2ZodhZtanzJs3b1VEjKou71cJpK2tjY6OjmaHYWbWp0iqPiMF4F1YZmZWkhOImZmV4gRiZmalOIGYmVkpTiBmZlaKE4iZmZXiBGJmZqU4gZiZWSlOIGZmVooTiJmZleIEYmZmpTiBmJlZKU4gZmZWihOImZmV4gRiZmalOIGYmVkpTiBmZlaKE4iZmZXiBGJmZqU4gZiZWSlOIGZmVooTiJmZleIEYmZmpTiBmJlZKU4gZmZWSlMTiKQpkh6QtFjS9Br1gyTdkOt/L6mtqn68pPWSPtFbMZuZWdK0BCJpAHAJcDgwEThW0sSqZicBayJiD+AC4Lyq+q8CP210rGZm9nLN3AKZBCyOiL9ExLPA9cCRVW2OBK7O098D3ipJAJKOAh4CFvRSvGZmVtDMBLIzsKQwvzSX1WwTERuAdcAOkrYF/gM4q7sXkTRNUoekjpUrV/ZI4GZm1ncH0c8ELoiI9d01jIgZEdEeEe2jRo1qfGRmZv3EwCa+9jJgXGF+bC6r1WappIHAMOAJYD/gaEnnA9sDL0j6e0Rc3PiwzcwMmptA5gITJO1KShTHAMdVtZkJTAXuBI4GbouIAN5caSDpTGC9k4eZWe9qWgKJiA2STgFmAQOAKyNigaSzgY6ImAlcAVwjaTGwmpRkzMysBSj9oO8f2tvbo6Ojo9lhmJn1KZLmRUR7dXlfHUQ3M7MmcwIxM7NSnEDMzKwUJxAzMyvFCcTMzEpxAjEzs1KcQMzMrBQnEDMzK8UJxMzMSnECMTOzUpxAzMysFCcQMzMrxQnEzMxKcQIxM7NSnEDMzKwUJxAzMyvFCcTMzEpxAjEzs1KcQMzMrBQnEDMzK8UJxMzMSnECMTOzUpxAzMysFCcQMzMrxQnEzMxKcQIxM7NSnEDMzKwUJxAzMyvFCcTMzEpxAjEzs1KamkAkTZH0gKTFkqbXqB8k6YZc/3tJbbn8EEnzJN2b7w/u7djNzPq7piUQSQOAS4DDgYnAsZImVjU7CVgTEXsAFwDn5fJVwDsi4rXAVOCa3onazMwqmrkFMglYHBF/iYhngeuBI6vaHAlcnae/B7xVkiLinoj4ay5fAAyRNKhXojYzM6C5CWRnYElhfmkuq9kmIjYA64Adqtq8C7g7Ip5pUJxmZlbDwGYHsDkk7UXarXVoF22mAdMAxo8f30uRmZlt+Zq5BbIMGFeYH5vLaraRNBAYBjyR58cCNwPvi4gHO3uRiJgREe0R0T5q1KgeDN/MrH9rZgKZC0yQtKukrYFjgJlVbWaSBskBjgZui4iQtD3wY2B6RPym1yI2M7MXNS2B5DGNU4BZwELgxohYIOlsSf+cm10B7CBpMfDvQOVQ31OAPYDPSZqfbzv2chfMzPo1RUSzY+g17e3t0dHR0ewwzMz6FEnzIqK9utz/RDczs1KcQMzMrBQnEDMzK8UJxMzMSuk2gUjaTdKPJK2S9LikH0rarTeCMzOz1lXPFsh3gBuBVwFjgO8C1zUyKDMza331JJBtIuKaiNiQb98GBjc6MDMza231nAvrp/laHdcDAbwX+ImkEQARsbqB8ZmZWYuqJ4G8J9+fXFV+DCmheDzEzKwf6jaBRMSuvRGImZn1LfUchbWNpM9KmpHnJ0h6e+NDMzOzVlbPIPq3gGeBN+b5ZcA5DYvIzMz6hHoSyO4RcT7wHEBEPA2ooVGZmVnLqyeBPCtpCGnAHEm7A758rJlZP1fPUVhnAj8Dxkm6FngT8K+NDMrMzFpfPUdh3SppHrA/adfVqRGxquGRmZlZS6vnKKw5EfFERPw4Im6JiFWS5vRGcGZm1ro63QKRNBjYBhgpaTgvDZxvB+zcC7GZmVkL62oX1snAaaQTKM7jpQTyJHBxg+MyM7MW12kCiYiLgIskfSQivt6LMZmZWR9Qz2G8KyQNBcj/SP++pH0aHJeZmbW4ehLIf0bEU5IOBCYDVwDfaGxYZmbW6upJIM/n+7cBMyLix8DWjQvJzMz6gnoSyDJJl/PSdUAG1fk4MzPbgtWTCN4DzAIOi4i1wAjgkw2NyszMWl49/0R/Gvh+YX45sLyRQZmZWevzrigzMyvFCcTMzEqpK4FI2kXS5Dw9pPK/EDMz67/qOZniB4HvAZfnorHADxoZlJmZtb56tkA+TLoGyJMAEbEI2LGRQZmZWeurJ4E8ExHPVmYkDSRfnXBzSZoi6QFJiyVNr1E/SNINuf73ktoKdafn8gckHdYT8ZiZWf3qSSC/kPRpYIikQ4DvAj/a3BeWNAC4BDgcmAgcK2liVbOTgDURsQdwAXBefuxE4BhgL2AKcGl+PjMz6yX1XNJ2OmlFfi/pFO8/Ab7ZA689CVgcEX8BkHQ9cCTwp0KbI0mX1IU0DnOxJOXy6yPiGeAhSYvz893ZA3G9zFk/WsCf/vpkI57azKzhJo7ZjjPesVePP289fyR8Afj/+daTdgaWFOaXAvt11iYiNkhaB+yQy39X9diaF7mSNA2YBjB+/PgeCdzMzOpIIJLu5eVjHuuADuCciHiiEYH1lIiYAcwAaG9vLzV204jMbWbW19WzC+unpDPyfifPH0O61O0K4CrgHSVfexkwrjA/NpfVarM0D94PA56o87FmZtZA9SSQyRFRvIDUvZLujoh9JB2/Ga89F5ggaVfSyv8Y4LiqNjOBqaSxjaOB2yIiJM0EviPpq6RL7k4A7tqMWMzMbBPVk0AGSJoUEXcBSNoXqBzxtKHsC+cxjVNIZ/odAFwZEQsknQ10RMRM0sWrrsmD5KtJSYbc7kbSgPsG4MMR8XzNFzIzs4ZQRNfDAjlhXAlsC4j0h8IPAAuAt0XEjY0Osqe0t7dHR0dHs8MwM+tTJM2LiPbq8nqOwpoLvFbSsDy/rlDdZ5KHmZn1rHp2YSHpbaQ/7Q1Of8OAiDi7gXGZmVmLq+dkipeRLmf7EdIurHcDuzQ4LjMza3H1nMrkjRHxPtIpRc4CDgBe3diwzMys1dWTQP6e75+WNAZ4DhjduJDMzKwvqGcM5EeStge+BNxN+ld6T5/WxMzM+pguE4ikrYA5EbEWuEnSLcDgqiOxzMysH+pyF1Y+keIlhflnnDzMzAzqGwOZI+ldqhy/a2ZmRn0J5GTSRaSelfSkpKck+eIYZmb9XD3/RB/aG4GYmVnfUs8fCSXpeEn/mefHSZrU+NDMzKyV1bML61LSnwcrp1pfT2Fg3czM+qd6/geyX772xz0AEbFG0tYNjsvMzFpcPVsgz0kaQL6sraRRwAsNjcrMzFpePQnka8DNwI6S/h/wa+ALDY3KzMxaXj1HYV0raR7wVtLZeI+KiIUNj8zMzFpatwlE0teA6yPCA+dmZvaienZhzQM+K+lBSV+W9LLLGpqZWf/TbQKJiKsj4ghgX+AB4DxJixoemZmZtbR6tkAq9gD2JF2N8P7GhGNmZn1FPf9EPz9vcZwN3Ae0R8Q7Gh6ZmZm1tHr+SPggcEBErGp0MGZm1nfUcxjv5ZKG5/NfDS6U/7KhkZmZWUur5zDeDwCnAmOB+cD+wJ3AwY0NzczMWlk9g+inko7AeiQi/gl4A7C2oVGZmVnLqyeB/D0i/g4gaVBE3A/8r8aGZWZmra6eQfSlkrYHfgDMlrQGeKSxYZmZWaurZxD9/+TJMyXdDgwDftbQqMzMrOXVswXyooj4RaMCMTOzvmVT/oneYySNkDRb0qJ8P7yTdlNzm0WSpuaybST9WNL9khZIOrd3ozczM2hSAgGmA3MiYgIwJ89vRNII4AxgP2AScEYh0Xw5IvYkHRH2JkmH907YZmZW0awEciRwdZ6+GjiqRpvDgNkRsToi1gCzgSkR8XRE3A4QEc8Cd5P+o2JmZr2oWQlkp4hYnqdXADvVaLMzsKQwvzSXvSgfHfYO0laMmZn1ok0aRN8Ukn4OvKpG1WeKMxERkqLE8w8ErgO+FhF/6aLdNGAawPjx4zf1ZczMrBMNSyARMbmzOkmPSRodEcsljQYer9FsGXBQYX4scEdhfgawKCIu7CaOGbkt7e3tm5yozMystmbtwpoJTM3TU4Ef1mgzCzg0n8hxOHBoLkPSOaT/o5zWC7GamVkNzUog5wKH5OuMTM7zSGqX9E2AiFgNfB6Ym29nR8RqSWNJu8EmAndLmp9P+GhmZr1IEf1nr057e3t0dHQ0Owwzsz5F0ryIaK8ub9YWiJmZ9XFOIGZmVooTiJmZleIEYmZmpTiBmJlZKU4gZmZWihOImZmV4gRiZmalOIGYmVkpTiBmZlaKE4iZmZXiBGJmZqU4gZiZWSlOIGZmVooTiJmZleIEYmZmpTiBmJlZKU4gZmZWihOImZmV4gRiZmalOIGYmVkpTiBmZlaKE4iZmZXiBGJmZqU4gZiZWSlOIGZmVooTiJmZleIEYmZmpTiBmJlZKU4gZmZWihOImZmV0pQEImmEpNmSFuX74Z20m5rbLJI0tUb9TEn3NT5iMzOr1qwtkOnAnIiYAMzJ8xuRNAI4A9gPmAScUUw0kt4JrO+dcM3MrFqzEsiRwNV5+mrgqBptDgNmR8TqiFgDzAamAEjaFvh34JxeiNXMzGpoVgLZKSKW5+kVwE412uwMLCnML81lAJ8HvgI83d0LSZomqUNSx8qVKzcjZDMzKxrYqCeW9HPgVTWqPlOciYiQFJvwvHsDu0fExyS1ddc+ImYAMwDa29vrfh0zM+tawxJIREzurE7SY5JGR8RySaOBx2s0WwYcVJgfC9wBHAC0S3qYFP+Oku6IiIMwM7Ne06xdWDOBylFVU4Ef1mgzCzhU0vA8eH4oMCsivhERYyKiDTgQ+LOTh5lZ72tWAjkXOETSImBynkdSu6RvAkTEatJYx9x8OzuXmZlZC1BE/xkWaG9vj46OjmaHYWbWp0iaFxHt1eX+J7qZmZXiBGJmZqU4gZiZWSlOIGZmVooTiJmZleIEYmZmpTiBmJlZKU4gZmZWihOImZmV4gRiZmalOIGYmVkpTiBmZlaKE4iZmZXiBGJmZqU4gZiZWSlOIGZmVooTiJmZleIEYmZmpTiBmJlZKU4gZmZWihOImZmV4gRiZmalOIGYmVkpTiBmZlaKIqLZMfQaSSuBR0o+fCSwqgfD6Qvc5/7Bfe4fNqfPu0TEqOrCfpVANoekjohob3Ycvcl97h/c5/6hEX32LiwzMyvFCcTMzEpxAqnfjGYH0ATuc//gPvcPPd5nj4GYmVkp3gIxM7NSnEDMzKwUJ5BuSJoi6QFJiyVNb3Y8PUnSlZIel3RfoWyEpNmSFuX74blckr6W34c/StqneZGXI2mcpNsl/UnSAkmn5vItuc+DJd0l6Q+5z2fl8l0l/T737QZJW+fyQXl+ca5va2b8m0PSAEn3SLolz2/RfZb0sKR7Jc2X1JHLGrpsO4F0QdIA4BLgcGAicKykic2NqkddBUypKpsOzImICcCcPA/pPZiQb9OAb/RSjD1pA/DxiJgI7A98OH+eW3KfnwEOjojXA3sDUyTtD5wHXBARewBrgJNy+5OANbn8gtyurzoVWFiY7w99/qeI2Lvwf4/GLtsR4VsnN+AAYFZh/nTg9GbH1cN9bAPuK8w/AIzO06OBB/L05cCxtdr11RvwQ+CQ/tJnYBvgbmA/0j+SB+byF5dzYBZwQJ4emNup2bGX6OvYvMI8GLgFUD/o88PAyKqyhi7b3gLp2s7AksL80ly2JdspIpbn6RXATnl6i3ov8m6KNwC/Zwvvc96VMx94HJgNPAisjYgNuUmxXy/2OdevA3bo3Yh7xIXAp4AX8vwObPl9DuBWSfMkTctlDV22B5aN1LZ8ERGStrjjvCVtC9wEnBYRT0p6sW5L7HNEPA/sLWl74GZgzyaH1FCS3g48HhHzJB3U7Hh60YERsUzSjsBsSfcXKxuxbHsLpGvLgHGF+bG5bEv2mKTRAPn+8Vy+RbwXkl5BSh7XRsT3c/EW3eeKiFgL3E7afbO9pMoPyGK/Xuxzrh8GPNHLoW6uNwH/LOlh4HrSbqyL2LL7TEQsy/ePk34oTKLBy7YTSNfmAhPy0RtbA8cAM5scU6PNBKbm6amkcYJK+fvy0Rv7A+sKm8Z9gtKmxhXAwoj4aqFqS+7zqLzlgaQhpDGfhaREcnRuVt3nyntxNHBb5J3kfUVEnB4RYyOijfSdvS0i/oUtuM+SXilpaGUaOBS4j0Yv280e+Gn1G3AE8GfSfuPPNDueHu7bdcBy4DnSPtCTSPt+5wCLgJ8DI3JbkY5IexC4F2hvdvwl+nsgaT/xH4H5+XbEFt7n1wH35D7fB3wul+8G3AUsBr4LDMrlg/P84ly/W7P7sJn9Pwi4ZUvvc+7bH/JtQWVd1ehl26cyMTOzUrwLy8zMSnECMTOzUpxAzMysFCcQMzMrxQnEzMxKcQIxK0HSb/N9m6Tjevi5P13rtcxajQ/jNdsM+VQZn4iIt2/CYwbGS+dkqlW/PiK27Yn4zBrJWyBmJUhanyfPBd6cr8HwsXziwi9Jmpuvs3Bybn+QpF9Jmgn8KZf9IJ/4bkHl5HeSzgWG5Oe7tvha+V/DX5J0X77uw3sLz32HpO9Jul/StSqe4MusQXwyRbPNM53CFkhOBOsiYl9Jg4DfSLo1t90HeE1EPJTn3x8Rq/MpRuZKuikipks6JSL2rvFa7yRd0+P1wMj8mF/mujcAewF/BX5DOh/Ur3u+u2Yv8RaIWc86lHSOofmkU8XvQLpoD8BdheQB8FFJfwB+Rzqx3QS6diBwXUQ8HxGPAb8A9i0899KIeIF0ipa2HumNWRe8BWLWswR8JCJmbVSYxkr+VjU/mXQho6cl3UE6J1NZzxSmn8ffbesF3gIx2zxPAUML87OAD+XTxiPp1fnsqNWGkS6j+rSkPUmX2K14rvL4Kr8C3pvHWUYBbyGd/M+sKfwrxWzz/BF4Pu+Kuop03Yk24O48kL0SOKrG434G/JukhaTLif6uUDcD+KOkuyOdhrziZtK1PP5AOqvwpyJiRU5AZr3Oh/GamVkp3oVlZmalOIGYmVkpTiBmZlaKE4iZmZXiBGJmZqU4gZiZWSlOIGZmVsr/ANUrBlWPSvnVAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "plt.figure()\n",
    "plt.plot(average_reward_list)\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('average reward')\n",
    "plt.title('Question 2 Average Rewards at Each Iteration')\n",
    "plt.savefig('Question_2.png')\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(average_step_list)\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('average step')\n",
    "plt.title('Question 2 Average Steps at Each Iteration')\n",
    "plt.savefig('Question_2.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('p2_policy.pkl', 'wb') as pickle_file:\n",
    "    pickle.dump(policy_network, pickle_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "options= \n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "action:  tensor([ 1.0000, -1.0000], grad_fn=<TanhBackward>)\n",
      "Finished in 150 steps\n"
     ]
    }
   ],
   "source": [
    "# load policy\n",
    "# with open('p2_policy.pkl', 'rb') as pickle_file:\n",
    "#     policy_network = pickle.load(pickle_file)\n",
    "\n",
    "# test policy\n",
    "env = gym.make('modified_gym_env:ReacherPyBulletEnv-v1', rand_init=False)\n",
    "# env.render()\n",
    "state = env.reset()\n",
    "done = False\n",
    "steps = 0\n",
    "\n",
    "while not done:\n",
    "    # TODO: do not sample here\n",
    "    action, log_prob = choose_action(policy_network, state, eval_policy=True)\n",
    "    state_next, reward, done, _ = env.step(action)\n",
    "    steps += 1\n",
    "\n",
    "print('Finished in {} steps'.format(steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
