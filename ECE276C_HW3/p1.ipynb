{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   }
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ECE 276C HW3 P1\n",
    "Mingwei Xu A53270271"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Policy Network using MLP\n",
    "    \"\"\"\n",
    "    def __init__(self, env):\n",
    "        \"\"\"\n",
    "        :param env: object, gym environment\n",
    "        \"\"\"\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        # get state space and action space dimension\n",
    "        self.state_space_n = env.observation_space.shape[0]\n",
    "        self.action_space_n = env.action_space.n\n",
    "\n",
    "        # define layers\n",
    "        self.l1 = nn.Linear(self.state_space_n, 64)\n",
    "        self.dropout = nn.Dropout(p=0.6)\n",
    "        self.l2 = nn.Linear(64, self.action_space_n)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Feed forward\n",
    "        \n",
    "        :param x: np array, state\n",
    "        :return: tensor, softmax probability of action\n",
    "        \"\"\"\n",
    "        # build neural network\n",
    "        network = nn.Sequential(\n",
    "            self.l1,\n",
    "            self.dropout,\n",
    "            self.l2,\n",
    "            nn.Softmax(dim=-1))\n",
    "        return network(torch.FloatTensor(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<generator object Module.parameters at 0x1112689a8>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "state = env.reset()\n",
    "policy_network = PolicyNetwork(env)\n",
    "policy_network.forward(state)   # TODO: test\n",
    "policy_network.parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Choose Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(policy_network, state):\n",
    "    \"\"\"\n",
    "    Choose action according to policy on given state\n",
    "\n",
    "    :param policy_network: object, policy network\n",
    "    :param state: np array, state\n",
    "    :returns: int, action; tensor, log probability\n",
    "    \"\"\"\n",
    "    probs = policy_network.forward(state)\n",
    "    m = Categorical(probs)\n",
    "    action = m.sample()\n",
    "    log_prob = m.log_prob(action)\n",
    "    return action.item(), log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(0, tensor(-0.7439, grad_fn=<SqueezeBackward1>))"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "choose_action(policy_network, state)    # TODO: test"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reinforce(env, policy_network, batch_size=500, num_episodes=200, lr=0.01, gamma=0.99):\n",
    "    \"\"\"\n",
    "    Policy gradient training using reinforce method\n",
    "\n",
    "    :param env: object, gym environment\n",
    "    :param policy_network: object, policy network\n",
    "    :param batch_size: int, batch size\n",
    "    :param num_episodes: int, number of episodes\n",
    "    :param lr: float, learning rate\n",
    "    :param gamma: float (0~1), discount factor\n",
    "    \"\"\"\n",
    "    # TODO: setup place holders\n",
    "    batch_loss_sum = 0  # sum of batch loss over episodes\n",
    "\n",
    "    # define optimizer\n",
    "    optimizer = optim.Adam(policy_network.parameters(), \n",
    "                           lr=lr)\n",
    "\n",
    "    # train\n",
    "    for i in range(num_episodes):\n",
    "        # setup placeholders\n",
    "        batch_states = []\n",
    "        batch_actions = []\n",
    "        batch_rewards = []\n",
    "        log_prob_list = []\n",
    "\n",
    "        # reset environment\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "\n",
    "        for j in range(batch_size):\n",
    "            # step\n",
    "            action, log_prob = choose_action(policy_network, state)\n",
    "            state_next, reward, done, _ = env.step(action)\n",
    "\n",
    "            # store data\n",
    "            batch_states.append(state)\n",
    "            batch_actions.append(action)\n",
    "            batch_rewards.append(reward)\n",
    "            log_prob_list.append(log_prob)\n",
    "\n",
    "            # move on\n",
    "            if done:\n",
    "                state = env.reset()\n",
    "            else:\n",
    "                state = state_next\n",
    "\n",
    "        # finish batch\n",
    "        batch_log_prob_sum = sum(log_prob_list)     # log likelihood sum, this is tensor\n",
    "        batch_discounted_return = sum(torch.FloatTensor(np.array(batch_rewards) *\n",
    "                                                        np.array([gamma ** t for t in range(1, len(batch_rewards) + 1)])))  # G(t), this is tensor\n",
    "        batch_loss = batch_discounted_return * batch_log_prob_sum   # calculate batch loss\n",
    "        # batch_loss_sum += batch_loss    # update batch loss sum over episodes\n",
    "        # loss = batch_loss_sum / (i + 1)\n",
    "        loss = batch_loss\n",
    "        print('Episode {} loss: {}, average reward: {}'.format(i, loss.item(), np.mean(batch_rewards)))\n",
    "\n",
    "        # update policy\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.zero_grad()\n",
    "        optimizer.step()\n",
    "        \n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Episode 0 loss: -32888.05859375, average reward: 1.0\nEpisode 1 loss: -32674.974609375, average reward: 1.0\nEpisode 2 loss: -32832.49609375, average reward: 1.0\nEpisode 3 loss: -33039.40234375, average reward: 1.0\nEpisode 4 loss: -33060.96875, average reward: 1.0\nEpisode 5 loss: -33032.546875, average reward: 1.0\nEpisode 6 loss: -33053.53125, average reward: 1.0\nEpisode 7 loss: -33065.30859375, average reward: 1.0\nEpisode 8 loss: -33031.04296875, average reward: 1.0\nEpisode 9 loss: -33104.64453125, average reward: 1.0\nEpisode 10 loss: -33085.28515625, average reward: 1.0\nEpisode 11 loss: -33068.640625, average reward: 1.0\nEpisode 12 loss: -33035.86328125, average reward: 1.0\nEpisode 13 loss: -33041.13671875, average reward: 1.0\nEpisode 14 loss: -33056.3203125, average reward: 1.0\nEpisode 15 loss: -33114.8046875, average reward: 1.0\nEpisode 16 loss: -33082.13671875, average reward: 1.0\nEpisode 17 loss: -33075.76171875, average reward: 1.0\nEpisode 18 loss: -33095.6640625, average reward: 1.0\nEpisode 19 loss: -33117.4609375, average reward: 1.0\nEpisode 20 loss: -33118.8671875, average reward: 1.0\nEpisode 21 loss: -33109.75, average reward: 1.0\nEpisode 22 loss: -33103.19140625, average reward: 1.0\nEpisode 23 loss: -33103.7734375, average reward: 1.0\nEpisode 24 loss: -33126.375, average reward: 1.0\nEpisode 25 loss: -33108.9609375, average reward: 1.0\nEpisode 26 loss: -33102.53125, average reward: 1.0\nEpisode 27 loss: -33095.109375, average reward: 1.0\nEpisode 28 loss: -33076.1796875, average reward: 1.0\nEpisode 29 loss: -33063.51953125, average reward: 1.0\nEpisode 30 loss: -33070.78125, average reward: 1.0\nEpisode 31 loss: -33058.81640625, average reward: 1.0\nEpisode 32 loss: -33055.84765625, average reward: 1.0\nEpisode 33 loss: -33072.96875, average reward: 1.0\nEpisode 34 loss: -33084.1875, average reward: 1.0\nEpisode 35 loss: -33081.0078125, average reward: 1.0\nEpisode 36 loss: -33068.69140625, average reward: 1.0\nEpisode 37 loss: -33073.38671875, average reward: 1.0\nEpisode 38 loss: -33054.13671875, average reward: 1.0\nEpisode 39 loss: -33048.2109375, average reward: 1.0\nEpisode 40 loss: -33051.9296875, average reward: 1.0\nEpisode 41 loss: -33054.5390625, average reward: 1.0\nEpisode 42 loss: -33042.8984375, average reward: 1.0\nEpisode 43 loss: -33036.55859375, average reward: 1.0\nEpisode 44 loss: -33024.65625, average reward: 1.0\nEpisode 45 loss: -33014.53125, average reward: 1.0\nEpisode 46 loss: -33027.109375, average reward: 1.0\nEpisode 47 loss: -33008.12109375, average reward: 1.0\nEpisode 48 loss: -33026.34375, average reward: 1.0\nEpisode 49 loss: -33017.41796875, average reward: 1.0\nEpisode 50 loss: -33027.109375, average reward: 1.0\nEpisode 51 loss: -33017.08203125, average reward: 1.0\nEpisode 52 loss: -33010.484375, average reward: 1.0\nEpisode 53 loss: -33000.5078125, average reward: 1.0\nEpisode 54 loss: -32990.58984375, average reward: 1.0\nEpisode 55 loss: -32988.98046875, average reward: 1.0\nEpisode 56 loss: -32983.94140625, average reward: 1.0\nEpisode 57 loss: -32975.890625, average reward: 1.0\nEpisode 58 loss: -32978.7421875, average reward: 1.0\nEpisode 59 loss: -32966.17578125, average reward: 1.0\nEpisode 60 loss: -32972.5703125, average reward: 1.0\nEpisode 61 loss: -32972.8984375, average reward: 1.0\nEpisode 62 loss: -32981.83203125, average reward: 1.0\nEpisode 63 loss: -32976.51953125, average reward: 1.0\nEpisode 64 loss: -32970.0625, average reward: 1.0\nEpisode 65 loss: -32962.7734375, average reward: 1.0\nEpisode 66 loss: -32956.60546875, average reward: 1.0\nEpisode 67 loss: -32958.5078125, average reward: 1.0\nEpisode 68 loss: -32964.36328125, average reward: 1.0\nEpisode 69 loss: -32978.09765625, average reward: 1.0\nEpisode 70 loss: -32980.34765625, average reward: 1.0\nEpisode 71 loss: -32979.29296875, average reward: 1.0\nEpisode 72 loss: -32988.0703125, average reward: 1.0\nEpisode 73 loss: -32975.77734375, average reward: 1.0\nEpisode 74 loss: -32964.93359375, average reward: 1.0\nEpisode 75 loss: -32963.11328125, average reward: 1.0\nEpisode 76 loss: -32964.140625, average reward: 1.0\nEpisode 77 loss: -32954.34765625, average reward: 1.0\nEpisode 78 loss: -32958.1328125, average reward: 1.0\nEpisode 79 loss: -32962.23046875, average reward: 1.0\nEpisode 80 loss: -32968.05078125, average reward: 1.0\nEpisode 81 loss: -32953.62109375, average reward: 1.0\nEpisode 82 loss: -32946.73046875, average reward: 1.0\nEpisode 83 loss: -32942.21484375, average reward: 1.0\nEpisode 84 loss: -32940.4921875, average reward: 1.0\nEpisode 85 loss: -32929.5625, average reward: 1.0\nEpisode 86 loss: -32929.29296875, average reward: 1.0\nEpisode 87 loss: -32932.62109375, average reward: 1.0\nEpisode 88 loss: -32933.0234375, average reward: 1.0\nEpisode 89 loss: -32937.41015625, average reward: 1.0\nEpisode 90 loss: -32940.703125, average reward: 1.0\nEpisode 91 loss: -32948.53515625, average reward: 1.0\nEpisode 92 loss: -32943.3671875, average reward: 1.0\nEpisode 93 loss: -32941.59765625, average reward: 1.0\nEpisode 94 loss: -32935.1953125, average reward: 1.0\nEpisode 95 loss: -32929.40234375, average reward: 1.0\nEpisode 96 loss: -32928.44921875, average reward: 1.0\nEpisode 97 loss: -32938.09765625, average reward: 1.0\nEpisode 98 loss: -32942.50390625, average reward: 1.0\nEpisode 99 loss: -32945.109375, average reward: 1.0\nEpisode 100 loss: -32941.1796875, average reward: 1.0\n"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-92edb07e94f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mreinforce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_network\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.99\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-5f48fad127b5>\u001b[0m in \u001b[0;36mreinforce\u001b[0;34m(env, policy_network, batch_size, num_episodes, lr, gamma)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;31m# update policy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    148\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \"\"\"\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "reinforce(env, policy_network, batch_size=500, num_episodes=200, gamma=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}