{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   }
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ECE 276C HW3 P1\n",
    "Mingwei Xu A53270271"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Policy Network using MLP\n",
    "    \"\"\"\n",
    "    def __init__(self, env):\n",
    "        \"\"\"\n",
    "        :param env: object, gym environment\n",
    "        \"\"\"\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        # get state space and action space dimension\n",
    "        self.state_space_n = env.observation_space.shape[0]\n",
    "        self.action_space_n = env.action_space.n\n",
    "\n",
    "        # define layers\n",
    "        self.l1 = nn.Linear(self.state_space_n, 64)\n",
    "        self.dropout = nn.Dropout(p=0.6)\n",
    "        self.l2 = nn.Linear(64, self.action_space_n)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Feed forward\n",
    "        \n",
    "        :param x: np array, state\n",
    "        :return: tensor, softmax probability of action\n",
    "        \"\"\"\n",
    "        # build neural network\n",
    "        network = nn.Sequential(\n",
    "            self.l1,\n",
    "            self.dropout,\n",
    "            self.l2,\n",
    "            nn.Softmax(dim=-1))\n",
    "        return network(torch.FloatTensor(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<generator object Module.parameters at 0x123bcb9a8>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "state = env.reset()\n",
    "policy_network = PolicyNetwork(env)\n",
    "policy_network.forward(state)   # TODO: test\n",
    "policy_network.parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Choose Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(policy_network, state):\n",
    "    \"\"\"\n",
    "    Choose action according to policy on given state\n",
    "\n",
    "    :param policy_network: object, policy network\n",
    "    :param state: np array, state\n",
    "    :returns: int, action; tensor, log probability\n",
    "    \"\"\"\n",
    "    probs = policy_network.forward(state)\n",
    "    m = Categorical(probs)\n",
    "    action = m.sample()\n",
    "    log_prob = m.log_prob(action)\n",
    "    return action.item(), log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(0, tensor(-0.8065, grad_fn=<SqueezeBackward1>))"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "choose_action(policy_network, state)    # TODO: test"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reinforce(env, policy_network, batch_size=500, num_episodes=200, lr=0.01, gamma=0.99):\n",
    "    \"\"\"\n",
    "    Policy gradient training using reinforce method\n",
    "\n",
    "    :param env: object, gym environment\n",
    "    :param policy_network: object, policy network\n",
    "    :param batch_size: int, batch size\n",
    "    :param num_episodes: int, number of episodes\n",
    "    :param lr: float, learning rate\n",
    "    :param gamma: float (0~1), discount factor\n",
    "    \"\"\"\n",
    "    # TODO: setup place holders\n",
    "    batch_loss_sum = 0  # sum of batch loss over episodes\n",
    "\n",
    "    # define optimizer\n",
    "    optimizer = optim.Adam(policy_network.parameters(), \n",
    "                           lr=lr)\n",
    "\n",
    "    # train\n",
    "    for i in range(num_episodes):\n",
    "        # setup placeholders\n",
    "        batch_states = []\n",
    "        batch_actions = []\n",
    "        batch_rewards = []\n",
    "        log_prob_list = []\n",
    "\n",
    "        # reset environment\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "\n",
    "        for j in range(batch_size):\n",
    "            # step\n",
    "            action, log_prob = choose_action(policy_network, state)\n",
    "            state_next, reward, done, _ = env.step(action)\n",
    "            # env.render()\n",
    "\n",
    "            # store data\n",
    "            batch_states.append(state)\n",
    "            batch_actions.append(action)\n",
    "            batch_rewards.append(reward)\n",
    "            log_prob_list.append(log_prob)\n",
    "\n",
    "            # move on\n",
    "            if done:\n",
    "                break   # TODO\n",
    "                state = env.reset()\n",
    "            else:\n",
    "                state = state_next\n",
    "\n",
    "        # finish batch\n",
    "        batch_log_prob_sum = sum(log_prob_list)     # log likelihood sum, this is tensor\n",
    "        batch_discounted_return = sum(torch.FloatTensor(np.array(batch_rewards) *\n",
    "                                                        np.array([gamma ** t for t in range(1, len(batch_rewards) + 1)])))  # G(t), this is tensor\n",
    "        batch_loss = batch_discounted_return * batch_log_prob_sum   # calculate batch loss\n",
    "        batch_loss_sum += batch_loss    # update batch loss sum over episodes\n",
    "        loss = batch_loss_sum / (i + 1)\n",
    "        print('Episode {} loss: {}, steps: {}, average reward: {}'.format(i, loss.item(), j, np.mean(batch_rewards)))\n",
    "\n",
    "        # update policy\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.zero_grad()\n",
    "        optimizer.step()\n",
    "        \n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Episode 0 loss: -344.17498779296875, steps: 24, average reward: 1.0\nEpisode 1 loss: -214.38392639160156, steps: 11, average reward: 1.0\nEpisode 2 loss: -263.3569641113281, steps: 24, average reward: 1.0\nEpisode 3 loss: -314.05859375, steps: 27, average reward: 1.0\nEpisode 4 loss: -419.3036193847656, steps: 39, average reward: 1.0\nEpisode 5 loss: -374.4591979980469, steps: 14, average reward: 1.0\nEpisode 6 loss: -462.47503662109375, steps: 41, average reward: 1.0\nEpisode 7 loss: -425.1252136230469, steps: 16, average reward: 1.0\nEpisode 8 loss: -387.8833923339844, steps: 10, average reward: 1.0\nEpisode 9 loss: -395.8592224121094, steps: 26, average reward: 1.0\nEpisode 10 loss: -383.96417236328125, steps: 20, average reward: 1.0\nEpisode 11 loss: -493.6012268066406, steps: 57, average reward: 1.0\nEpisode 12 loss: -491.3536682128906, steps: 27, average reward: 1.0\nEpisode 13 loss: -495.4492492675781, steps: 29, average reward: 1.0\nEpisode 14 loss: -480.7867736816406, steps: 20, average reward: 1.0\nEpisode 15 loss: -456.69464111328125, steps: 11, average reward: 1.0\nEpisode 16 loss: -441.23187255859375, steps: 16, average reward: 1.0\nEpisode 17 loss: -424.71881103515625, steps: 15, average reward: 1.0\nEpisode 18 loss: -481.2038879394531, steps: 55, average reward: 1.0\nEpisode 19 loss: -481.27752685546875, steps: 28, average reward: 1.0\nEpisode 20 loss: -470.3784484863281, steps: 19, average reward: 1.0\nEpisode 21 loss: -456.63897705078125, steps: 17, average reward: 1.0\nEpisode 22 loss: -444.025390625, steps: 15, average reward: 1.0\nEpisode 23 loss: -429.5783386230469, steps: 11, average reward: 1.0\nEpisode 24 loss: -423.6417236328125, steps: 21, average reward: 1.0\nEpisode 25 loss: -421.3302307128906, steps: 23, average reward: 1.0\nEpisode 26 loss: -414.2571716308594, steps: 18, average reward: 1.0\nEpisode 27 loss: -434.94512939453125, steps: 40, average reward: 1.0\nEpisode 28 loss: -421.6256103515625, steps: 8, average reward: 1.0\nEpisode 29 loss: -434.1842956542969, steps: 35, average reward: 1.0\nEpisode 30 loss: -457.4512023925781, steps: 45, average reward: 1.0\nEpisode 31 loss: -479.2393798828125, steps: 44, average reward: 1.0\nEpisode 32 loss: -481.2560119628906, steps: 31, average reward: 1.0\nEpisode 33 loss: -471.54095458984375, steps: 15, average reward: 1.0\nEpisode 34 loss: -464.5986328125, steps: 18, average reward: 1.0\nEpisode 35 loss: -456.3122863769531, steps: 15, average reward: 1.0\nEpisode 36 loss: -482.4453125, steps: 51, average reward: 1.0\nEpisode 37 loss: -495.43450927734375, steps: 40, average reward: 1.0\nEpisode 38 loss: -493.0411682128906, steps: 25, average reward: 1.0\nEpisode 39 loss: -482.50604248046875, steps: 9, average reward: 1.0\nEpisode 40 loss: -479.9846496582031, steps: 24, average reward: 1.0\nEpisode 41 loss: -471.9256896972656, steps: 14, average reward: 1.0\nEpisode 42 loss: -463.6034851074219, steps: 13, average reward: 1.0\nEpisode 43 loss: -468.5611267089844, steps: 34, average reward: 1.0\nEpisode 44 loss: -460.7647705078125, steps: 13, average reward: 1.0\nEpisode 45 loss: -512.9523315429688, steps: 76, average reward: 1.0\nEpisode 46 loss: -504.0328369140625, steps: 11, average reward: 1.0\nEpisode 47 loss: -496.1436462402344, steps: 13, average reward: 1.0\nEpisode 48 loss: -493.0705261230469, steps: 23, average reward: 1.0\nEpisode 49 loss: -486.2999572753906, steps: 15, average reward: 1.0\nEpisode 50 loss: -490.6321105957031, steps: 35, average reward: 1.0\nEpisode 51 loss: -487.0683288574219, steps: 21, average reward: 1.0\nEpisode 52 loss: -479.9171447753906, steps: 12, average reward: 1.0\nEpisode 53 loss: -475.7486572265625, steps: 20, average reward: 1.0\nEpisode 54 loss: -473.2877502441406, steps: 23, average reward: 1.0\nEpisode 55 loss: -487.77972412109375, steps: 47, average reward: 1.0\nEpisode 56 loss: -480.7674560546875, steps: 11, average reward: 1.0\nEpisode 57 loss: -473.3960266113281, steps: 8, average reward: 1.0\nEpisode 58 loss: -466.8804626464844, steps: 11, average reward: 1.0\nEpisode 59 loss: -460.2365417480469, steps: 9, average reward: 1.0\nEpisode 60 loss: -455.469970703125, steps: 15, average reward: 1.0\nEpisode 61 loss: -450.74603271484375, steps: 15, average reward: 1.0\nEpisode 62 loss: -444.8311462402344, steps: 11, average reward: 1.0\nEpisode 63 loss: -445.7750244140625, steps: 29, average reward: 1.0\nEpisode 64 loss: -471.7536315917969, steps: 65, average reward: 1.0\nEpisode 65 loss: -467.2314758300781, steps: 16, average reward: 1.0\nEpisode 66 loss: -463.06988525390625, steps: 16, average reward: 1.0\nEpisode 67 loss: -457.614013671875, steps: 12, average reward: 1.0\nEpisode 68 loss: -453.00677490234375, steps: 14, average reward: 1.0\nEpisode 69 loss: -449.2978210449219, steps: 16, average reward: 1.0\nEpisode 70 loss: -444.3070983886719, steps: 12, average reward: 1.0\nEpisode 71 loss: -442.5308837890625, steps: 22, average reward: 1.0\nEpisode 72 loss: -441.86279296875, steps: 25, average reward: 1.0\nEpisode 73 loss: -448.83074951171875, steps: 41, average reward: 1.0\nEpisode 74 loss: -463.2605285644531, steps: 55, average reward: 1.0\nEpisode 75 loss: -463.53228759765625, steps: 28, average reward: 1.0\nEpisode 76 loss: -464.1763000488281, steps: 28, average reward: 1.0\nEpisode 77 loss: -462.8507995605469, steps: 25, average reward: 1.0\nEpisode 78 loss: -458.596923828125, steps: 14, average reward: 1.0\nEpisode 79 loss: -455.269775390625, steps: 17, average reward: 1.0\nEpisode 80 loss: -451.6543273925781, steps: 15, average reward: 1.0\nEpisode 81 loss: -450.6046447753906, steps: 25, average reward: 1.0\nEpisode 82 loss: -446.1859130859375, steps: 10, average reward: 1.0\nEpisode 83 loss: -442.8417053222656, steps: 15, average reward: 1.0\nEpisode 84 loss: -442.1399841308594, steps: 24, average reward: 1.0\nEpisode 85 loss: -441.6528015136719, steps: 25, average reward: 1.0\nEpisode 86 loss: -437.5878601074219, steps: 11, average reward: 1.0\nEpisode 87 loss: -434.4377746582031, steps: 15, average reward: 1.0\nEpisode 88 loss: -438.0487060546875, steps: 36, average reward: 1.0\nEpisode 89 loss: -437.4591979980469, steps: 25, average reward: 1.0\nEpisode 90 loss: -435.1647033691406, steps: 18, average reward: 1.0\nEpisode 91 loss: -439.6479187011719, steps: 38, average reward: 1.0\nEpisode 92 loss: -437.37518310546875, steps: 18, average reward: 1.0\nEpisode 93 loss: -436.3288879394531, steps: 23, average reward: 1.0\nEpisode 94 loss: -433.8321533203125, steps: 17, average reward: 1.0\nEpisode 95 loss: -431.8714904785156, steps: 18, average reward: 1.0\nEpisode 96 loss: -431.7309875488281, steps: 27, average reward: 1.0\nEpisode 97 loss: -428.5750427246094, steps: 13, average reward: 1.0\nEpisode 98 loss: -425.8045654296875, steps: 15, average reward: 1.0\nEpisode 99 loss: -423.4862976074219, steps: 17, average reward: 1.0\nEpisode 100 loss: -424.86859130859375, steps: 30, average reward: 1.0\nEpisode 101 loss: -423.9583740234375, steps: 23, average reward: 1.0\nEpisode 102 loss: -423.38665771484375, steps: 23, average reward: 1.0\nEpisode 103 loss: -422.1734619140625, steps: 21, average reward: 1.0\nEpisode 104 loss: -422.60711669921875, steps: 27, average reward: 1.0\nEpisode 105 loss: -422.9093933105469, steps: 27, average reward: 1.0\nEpisode 106 loss: -420.1552429199219, steps: 13, average reward: 1.0\nEpisode 107 loss: -423.27227783203125, steps: 34, average reward: 1.0\nEpisode 108 loss: -421.04583740234375, steps: 17, average reward: 1.0\nEpisode 109 loss: -417.94573974609375, steps: 11, average reward: 1.0\nEpisode 110 loss: -415.5668029785156, steps: 15, average reward: 1.0\nEpisode 111 loss: -422.9340515136719, steps: 47, average reward: 1.0\nEpisode 112 loss: -420.71636962890625, steps: 15, average reward: 1.0\nEpisode 113 loss: -423.1595153808594, steps: 34, average reward: 1.0\nEpisode 114 loss: -422.697021484375, steps: 25, average reward: 1.0\nEpisode 115 loss: -420.8754577636719, steps: 18, average reward: 1.0\nEpisode 116 loss: -419.0640563964844, steps: 17, average reward: 1.0\nEpisode 117 loss: -417.0675964355469, steps: 17, average reward: 1.0\nEpisode 118 loss: -417.24493408203125, steps: 27, average reward: 1.0\nEpisode 119 loss: -431.5872802734375, steps: 63, average reward: 1.0\nEpisode 120 loss: -429.0030212402344, steps: 13, average reward: 1.0\nEpisode 121 loss: -426.7669982910156, steps: 15, average reward: 1.0\nEpisode 122 loss: -425.2386169433594, steps: 20, average reward: 1.0\nEpisode 123 loss: -426.368896484375, steps: 31, average reward: 1.0\nEpisode 124 loss: -425.605224609375, steps: 22, average reward: 1.0\nEpisode 125 loss: -426.20458984375, steps: 28, average reward: 1.0\nEpisode 126 loss: -424.81884765625, steps: 19, average reward: 1.0\nEpisode 127 loss: -424.7066955566406, steps: 26, average reward: 1.0\nEpisode 128 loss: -422.4935302734375, steps: 14, average reward: 1.0\nEpisode 129 loss: -420.8705139160156, steps: 18, average reward: 1.0\nEpisode 130 loss: -419.0330810546875, steps: 16, average reward: 1.0\nEpisode 131 loss: -418.5673522949219, steps: 23, average reward: 1.0\nEpisode 132 loss: -417.3423156738281, steps: 20, average reward: 1.0\nEpisode 133 loss: -415.5276794433594, steps: 16, average reward: 1.0\nEpisode 134 loss: -416.8702697753906, steps: 32, average reward: 1.0\nEpisode 135 loss: -415.6285705566406, steps: 20, average reward: 1.0\nEpisode 136 loss: -415.7535400390625, steps: 27, average reward: 1.0\nEpisode 137 loss: -414.4903564453125, steps: 19, average reward: 1.0\nEpisode 138 loss: -412.3884582519531, steps: 12, average reward: 1.0\nEpisode 139 loss: -410.1099548339844, steps: 11, average reward: 1.0\nEpisode 140 loss: -417.89129638671875, steps: 53, average reward: 1.0\nEpisode 141 loss: -418.0950622558594, steps: 26, average reward: 1.0\nEpisode 142 loss: -417.5395202636719, steps: 23, average reward: 1.0\nEpisode 143 loss: -417.5365905761719, steps: 26, average reward: 1.0\nEpisode 144 loss: -416.1784362792969, steps: 17, average reward: 1.0\nEpisode 145 loss: -422.0826110839844, steps: 48, average reward: 1.0\nEpisode 146 loss: -420.2307434082031, steps: 14, average reward: 1.0\nEpisode 147 loss: -418.7481384277344, steps: 18, average reward: 1.0\nEpisode 148 loss: -420.623779296875, steps: 35, average reward: 1.0\nEpisode 149 loss: -421.22760009765625, steps: 29, average reward: 1.0\nEpisode 150 loss: -419.4261169433594, steps: 14, average reward: 1.0\nEpisode 151 loss: -417.1761474609375, steps: 10, average reward: 1.0\nEpisode 152 loss: -428.9598083496094, steps: 67, average reward: 1.0\nEpisode 153 loss: -426.55645751953125, steps: 8, average reward: 1.0\nEpisode 154 loss: -425.14459228515625, steps: 17, average reward: 1.0\nEpisode 155 loss: -424.1805419921875, steps: 21, average reward: 1.0\nEpisode 156 loss: -433.7406005859375, steps: 62, average reward: 1.0\nEpisode 157 loss: -443.26873779296875, steps: 61, average reward: 1.0\nEpisode 158 loss: -440.83343505859375, steps: 9, average reward: 1.0\nEpisode 159 loss: -441.3572692871094, steps: 31, average reward: 1.0\nEpisode 160 loss: -440.53948974609375, steps: 21, average reward: 1.0\nEpisode 161 loss: -441.12017822265625, steps: 30, average reward: 1.0\nEpisode 162 loss: -440.34466552734375, steps: 22, average reward: 1.0\nEpisode 163 loss: -449.93621826171875, steps: 62, average reward: 1.0\nEpisode 164 loss: -448.0955505371094, steps: 15, average reward: 1.0\nEpisode 165 loss: -445.853271484375, steps: 11, average reward: 1.0\nEpisode 166 loss: -443.9143981933594, steps: 13, average reward: 1.0\nEpisode 167 loss: -441.8573303222656, steps: 12, average reward: 1.0\nEpisode 168 loss: -440.9298400878906, steps: 22, average reward: 1.0\nEpisode 169 loss: -438.65478515625, steps: 8, average reward: 1.0\nEpisode 170 loss: -438.5858154296875, steps: 25, average reward: 1.0\nEpisode 171 loss: -436.95361328125, steps: 15, average reward: 1.0\nEpisode 172 loss: -435.4679260253906, steps: 16, average reward: 1.0\nEpisode 173 loss: -433.8531799316406, steps: 15, average reward: 1.0\nEpisode 174 loss: -440.45672607421875, steps: 54, average reward: 1.0\nEpisode 175 loss: -439.5123291015625, steps: 19, average reward: 1.0\nEpisode 176 loss: -438.1986083984375, steps: 18, average reward: 1.0\nEpisode 177 loss: -441.20465087890625, steps: 41, average reward: 1.0\nEpisode 178 loss: -443.5180358886719, steps: 38, average reward: 1.0\nEpisode 179 loss: -441.7138977050781, steps: 13, average reward: 1.0\nEpisode 180 loss: -444.67718505859375, steps: 41, average reward: 1.0\nEpisode 181 loss: -443.77081298828125, steps: 20, average reward: 1.0\nEpisode 182 loss: -442.81146240234375, steps: 19, average reward: 1.0\nEpisode 183 loss: -448.7529296875, steps: 53, average reward: 1.0\nEpisode 184 loss: -448.10650634765625, steps: 23, average reward: 1.0\nEpisode 185 loss: -446.49273681640625, steps: 14, average reward: 1.0\nEpisode 186 loss: -450.7154541015625, steps: 47, average reward: 1.0\nEpisode 187 loss: -449.7638244628906, steps: 22, average reward: 1.0\nEpisode 188 loss: -448.0400695800781, steps: 12, average reward: 1.0\nEpisode 189 loss: -446.410888671875, steps: 14, average reward: 1.0\nEpisode 190 loss: -445.0079040527344, steps: 16, average reward: 1.0\nEpisode 191 loss: -446.5797119140625, steps: 36, average reward: 1.0\nEpisode 192 loss: -445.3464660644531, steps: 18, average reward: 1.0\nEpisode 193 loss: -443.98974609375, steps: 17, average reward: 1.0\nEpisode 194 loss: -445.76751708984375, steps: 37, average reward: 1.0\nEpisode 195 loss: -444.3631896972656, steps: 15, average reward: 1.0\nEpisode 196 loss: -442.7315673828125, steps: 13, average reward: 1.0\nEpisode 197 loss: -442.15447998046875, steps: 22, average reward: 1.0\nEpisode 198 loss: -442.6728210449219, steps: 28, average reward: 1.0\nEpisode 199 loss: -441.9754943847656, steps: 21, average reward: 1.0\n"
    }
   ],
   "source": [
    "reinforce(env, policy_network, batch_size=500, num_episodes=200, gamma=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}